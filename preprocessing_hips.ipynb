{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from PyMO.pymo.parsers import BVHParser\n",
    "from PyMO.pymo.preprocessing import *\n",
    "from PyMO.pymo.viz_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e77ca62d6f0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pymo/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e77ca62d900>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pymo/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e77ca62db10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pymo/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pymo\n",
      "  Using cached pymo-0.2.0-py3-none-any.whl\n",
      "Requirement already satisfied: pymongo<4,>=2 in /root/.local/lib/python3.10/site-packages (from pymo) (3.13.0)\n",
      "Installing collected packages: pymo\n",
      "Successfully installed pymo-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pymo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/OMR-Research/pymo.git\n",
      "  Cloning https://github.com/OMR-Research/pymo.git to /tmp/pip-req-build-bfyq77qd\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/OMR-Research/pymo.git /tmp/pip-req-build-bfyq77qd\n",
      "Username for 'https://github.com': ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/OMR-Research/pymo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://****@github.com/iPERDance/pymo.git\n",
      "  Cloning https://****@github.com/iPERDance/pymo.git to /tmp/pip-req-build-0xaa93ng\n",
      "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/iPERDance/pymo.git' /tmp/pip-req-build-0xaa93ng\n",
      "  remote: Repository not found.\n",
      "  fatal: repository 'https://github.com/iPERDance/pymo.git/' not found\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[32m'https://****@github.com/iPERDance/pymo.git'\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/\u001b[0m\u001b[32mpip-req-build-0xaa93ng\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[32m'https://****@github.com/iPERDance/pymo.git'\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/\u001b[0m\u001b[32mpip-req-build-0xaa93ng\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://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@github.com/iPERDance/pymo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (1.21.5)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "\u001b[33m  WARNING: The scripts f2py and numpy-config are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_list = {\n",
    "    \"beat_joints\" : {\n",
    "        'Hips':         [6,6],\n",
    "        'Spine':        [3,9],\n",
    "        'Spine1':       [3,12],\n",
    "        'Spine2':       [3,15],\n",
    "        'Spine3':       [3,18],\n",
    "        'Neck':         [3,21],\n",
    "        'Neck1':        [3,24],\n",
    "        'Head':         [3,27],\n",
    "        'HeadEnd':      [3,30],\n",
    "\n",
    "        'RShoulder':    [3,33], \n",
    "        'RArm':         [3,36],\n",
    "        'RArm1':        [3,39],\n",
    "        'RHand':        [3,42],    \n",
    "        'RHandM1':      [3,45],\n",
    "        'RHandM2':      [3,48],\n",
    "        'RHandM3':      [3,51],\n",
    "        'RHandM4':      [3,54],\n",
    "\n",
    "        'RHandR':       [3,57],\n",
    "        'RHandR1':      [3,60],\n",
    "        'RHandR2':      [3,63],\n",
    "        'RHandR3':      [3,66],\n",
    "        'RHandR4':      [3,69],\n",
    "\n",
    "        'RHandP':       [3,72],\n",
    "        'RHandP1':      [3,75],\n",
    "        'RHandP2':      [3,78],\n",
    "        'RHandP3':      [3,81],\n",
    "        'RHandP4':      [3,84],\n",
    "\n",
    "        'RHandI':       [3,87],\n",
    "        'RHandI1':      [3,90],\n",
    "        'RHandI2':      [3,93],\n",
    "        'RHandI3':      [3,96],\n",
    "        'RHandI4':      [3,99],\n",
    "\n",
    "        'RHandT1':      [3,102],\n",
    "        'RHandT2':      [3,105],\n",
    "        'RHandT3':      [3,108],\n",
    "        'RHandT4':      [3,111],\n",
    "\n",
    "        'LShoulder':    [3,114], \n",
    "        'LArm':         [3,117],\n",
    "        'LArm1':        [3,120],\n",
    "        'LHand':        [3,123],    \n",
    "        'LHandM1':      [3,126],\n",
    "        'LHandM2':      [3,129],\n",
    "        'LHandM3':      [3,132],\n",
    "        'LHandM4':      [3,135],\n",
    "\n",
    "        'LHandR':       [3,138],\n",
    "        'LHandR1':      [3,141],\n",
    "        'LHandR2':      [3,144],\n",
    "        'LHandR3':      [3,147],\n",
    "        'LHandR4':      [3,150],\n",
    "\n",
    "        'LHandP':       [3,153],\n",
    "        'LHandP1':      [3,156],\n",
    "        'LHandP2':      [3,159],\n",
    "        'LHandP3':      [3,162],\n",
    "        'LHandP4':      [3,165],\n",
    "\n",
    "        'LHandI':       [3,168],\n",
    "        'LHandI1':      [3,171],\n",
    "        'LHandI2':      [3,174],\n",
    "        'LHandI3':      [3,177],\n",
    "        'LHandI4':      [3,180],\n",
    "\n",
    "        'LHandT1':      [3,183],\n",
    "        'LHandT2':      [3,186],\n",
    "        'LHandT3':      [3,189],\n",
    "        'LHandT4':      [3,192],\n",
    "\n",
    "        'RUpLeg':       [3,195],\n",
    "        'RLeg':         [3,198],\n",
    "        'RFoot':        [3,201],\n",
    "        'RFootF':       [3,204],\n",
    "        'RToeBase':     [3,207],\n",
    "        'RToeBaseEnd':  [3,210],\n",
    "\n",
    "        'LUpLeg':       [3,213],\n",
    "        'LLeg':         [3,216],\n",
    "        'LFoot':        [3,219],\n",
    "        'LFootF':       [3,222],\n",
    "        'LToeBase':     [3,225],\n",
    "        'LToeBaseEnd':  [3,228],\n",
    "        },\n",
    "\n",
    "    \"beat_full\" : {\n",
    "        'Hips':         3,\n",
    "        'Spine':        3,\n",
    "        'Spine1':       3,\n",
    "        'Spine2':       3,\n",
    "        'Spine3':       3,\n",
    "        'Neck':         3,\n",
    "        'Neck1':        3,\n",
    "        'Head':         3,\n",
    "        'HeadEnd':      3,\n",
    "\n",
    "        'RShoulder':    3,\n",
    "        'RArm':         3,\n",
    "        'RArm1':        3,\n",
    "        'RHand':        3,    \n",
    "        'RHandM1':      3,\n",
    "        'RHandM2':      3,\n",
    "        'RHandM3':      3,\n",
    "        'RHandM4':      3,\n",
    "\n",
    "        'RHandR':       3,\n",
    "        'RHandR1':      3,\n",
    "        'RHandR2':      3,\n",
    "        'RHandR3':      3,\n",
    "        'RHandR4':      3,\n",
    "\n",
    "        'RHandP':       3,\n",
    "        'RHandP1':      3,\n",
    "        'RHandP2':      3,\n",
    "        'RHandP3':      3,\n",
    "        'RHandP4':      3,\n",
    "\n",
    "        'RHandI':       3,\n",
    "        'RHandI1':      3,\n",
    "        'RHandI2':      3,\n",
    "        'RHandI3':      3,\n",
    "        'RHandI4':      3,\n",
    "\n",
    "        'RHandT1':      3,\n",
    "        'RHandT2':      3,\n",
    "        'RHandT3':      3,\n",
    "        'RHandT4':      3,\n",
    "\n",
    "        'LShoulder':    3, \n",
    "        'LArm':         3,\n",
    "        'LArm1':        3,\n",
    "        'LHand':        3,    \n",
    "        'LHandM1':      3,\n",
    "        'LHandM2':      3,\n",
    "        'LHandM3':      3,\n",
    "        'LHandM4':      3,\n",
    "\n",
    "        'LHandR':       3,\n",
    "        'LHandR1':      3,\n",
    "        'LHandR2':      3,\n",
    "        'LHandR3':      3,\n",
    "        'LHandR4':      3,\n",
    "\n",
    "        'LHandP':       3,\n",
    "        'LHandP1':      3,\n",
    "        'LHandP2':      3,\n",
    "        'LHandP3':      3,\n",
    "        'LHandP4':      3,\n",
    "\n",
    "        'LHandI':       3,\n",
    "        'LHandI1':      3,\n",
    "        'LHandI2':      3,\n",
    "        'LHandI3':      3,\n",
    "        'LHandI4':      3,\n",
    "\n",
    "        'LHandT1':      3,\n",
    "        'LHandT2':      3,\n",
    "        'LHandT3':      3,\n",
    "        'LHandT4':      3,\n",
    "\n",
    "        'RUpLeg':       3,\n",
    "        'RLeg':         3,\n",
    "        'RFoot':        3,\n",
    "        'RFootF':       3,\n",
    "        'RToeBase':     3,\n",
    "        'RToeBaseEnd':  3,\n",
    "\n",
    "        'LUpLeg':       3,\n",
    "        'LLeg':         3,\n",
    "        'LFoot':        3,\n",
    "        'LFootF':       3,\n",
    "        'LToeBase':     3,\n",
    "        'LToeBaseEnd':  3,\n",
    "    },\n",
    "    \n",
    "    \"beat_141\" : {\n",
    "            'Spine':       3 ,\n",
    "            'Neck':        3 ,\n",
    "            'Neck1':       3 ,\n",
    "            'RShoulder':   3 , \n",
    "            'RArm':        3 ,\n",
    "            'RArm1':       3 ,\n",
    "            'RHand':       3 ,    \n",
    "            'RHandM1':     3 ,\n",
    "            'RHandM2':     3 ,\n",
    "            'RHandM3':     3 ,\n",
    "            'RHandR':      3 ,\n",
    "            'RHandR1':     3 ,\n",
    "            'RHandR2':     3 ,\n",
    "            'RHandR3':     3 ,\n",
    "            'RHandP':      3 ,\n",
    "            'RHandP1':     3 ,\n",
    "            'RHandP2':     3 ,\n",
    "            'RHandP3':     3 ,\n",
    "            'RHandI':      3 ,\n",
    "            'RHandI1':     3 ,\n",
    "            'RHandI2':     3 ,\n",
    "            'RHandI3':     3 ,\n",
    "            'RHandT1':     3 ,\n",
    "            'RHandT2':     3 ,\n",
    "            'RHandT3':     3 ,\n",
    "            'LShoulder':   3 , \n",
    "            'LArm':        3 ,\n",
    "            'LArm1':       3 ,\n",
    "            'LHand':       3 ,    \n",
    "            'LHandM1':     3 ,\n",
    "            'LHandM2':     3 ,\n",
    "            'LHandM3':     3 ,\n",
    "            'LHandR':      3 ,\n",
    "            'LHandR1':     3 ,\n",
    "            'LHandR2':     3 ,\n",
    "            'LHandR3':     3 ,\n",
    "            'LHandP':      3 ,\n",
    "            'LHandP1':     3 ,\n",
    "            'LHandP2':     3 ,\n",
    "            'LHandP3':     3 ,\n",
    "            'LHandI':      3 ,\n",
    "            'LHandI1':     3 ,\n",
    "            'LHandI2':     3 ,\n",
    "            'LHandI3':     3 ,\n",
    "            'LHandT1':     3 ,\n",
    "            'LHandT2':     3 ,\n",
    "            'LHandT3':     3 ,\n",
    "        },\n",
    "    \n",
    "    \"beat_27\" : {\n",
    "            'Spine':       3 ,\n",
    "            'Neck':        3 ,\n",
    "            'Neck1':       3 ,\n",
    "            'RShoulder':   3 , \n",
    "            'RArm':        3 ,\n",
    "            'RArm1':       3 ,\n",
    "            'LShoulder':   3 , \n",
    "            'LArm':        3 ,\n",
    "            'LArm1':       3 ,     \n",
    "        },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_fps = 30\n",
    "ori_list = joint_list[\"beat_joints\"]\n",
    "target_list = joint_list[\"beat_full\"]\n",
    "ori_data_path = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_v0.2.1_hips/\"\n",
    "ori_data_path_npy = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_npy_hips/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data processing ...: 100% 1/1 [00:00<00:00, 229.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for speaker in tqdm(range(1,2), \"Data processing ...\"):\n",
    "    for f in os.listdir(ori_data_path+str(speaker)):\n",
    "        if(f.endswith('wav')):\n",
    "            audio_folder = os.path.join(ori_data_path_npy+str(speaker))\n",
    "            os.makedirs(audio_folder, exist_ok=True)\n",
    "            audioOut = os.path.join(ori_data_path_npy+str(speaker), f.replace('wav', 'npy'))\n",
    "            if(not os.path.exists(audioOut)):\n",
    "                a, sr = librosa.load(os.path.join(ori_data_path+str(speaker), f), sr=16000)\n",
    "                np.save(audioOut, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_fps: 30, reduce json 2, reduce bvh 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:01, 10.22it/s]\n"
     ]
    }
   ],
   "source": [
    "#calculate mean and build cache for data. \n",
    "target_fps = 30\n",
    "ori_list = joint_list[\"beat_joints\"]\n",
    "target_list = joint_list[\"beat_joints\"]\n",
    "ori_data_path = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_v0.2.1_hips/\"\n",
    "#wave cache from a = librosa.load(sr=16000) and np.save(a)\n",
    "ori_data_path_npy = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_npy_hips/\"\n",
    "ori_data_path_ann = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_v0.2.1_hips/\"\n",
    "cache_path = f\"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_{target_fps}_full_hips/\"\n",
    "reduce_factor_json = int(60/target_fps)\n",
    "reduce_factor_bvh = int(120/target_fps)\n",
    "print(f\"target_fps: {target_fps}, reduce json {reduce_factor_json}, reduce bvh {reduce_factor_bvh}\")\n",
    "speakers = sorted(os.listdir(ori_data_path),key=str,)\n",
    "\n",
    "npy_s_v = []\n",
    "npy_s_k = []\n",
    "json_s_v = []\n",
    "bvh_s_v = []\n",
    "\n",
    "load_type = \"train\"\n",
    "if not os.path.exists(f\"{cache_path}\"): \n",
    "    os.mkdir(cache_path)\n",
    "if not os.path.exists(f\"{cache_path}{load_type}/\"): \n",
    "    os.mkdir(f\"{cache_path}{load_type}/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/wave16k/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/bvh_rot/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/bvh_full/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/bvh_rot_vis/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/facial52/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/text/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/emo/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/sem/\")     \n",
    "\n",
    "for speaker in [1]: #range(1, 31):#replace to 1, 31 for all speakers\n",
    "    if speaker in []:\n",
    "        print(\"Skip: \", speaker)\n",
    "        # break\n",
    "    else:\n",
    "        all_data = os.listdir(ori_data_path_npy+str(speaker))\n",
    "        npy_all = []\n",
    "        json_all = []\n",
    "        bvh_all = []   \n",
    "        for ii, file in tqdm(enumerate(all_data)):\n",
    "            file = file[:-4]\n",
    "            npy_all.extend(list(np.load(f\"{ori_data_path_npy}/{file.split('_')[0]}/{file}.npy\")))\n",
    "\n",
    "        npy_all = np.array(npy_all)\n",
    "        npy_mean = np.mean(npy_all, axis=0)\n",
    "        npy_std = np.std(npy_all, axis=0)\n",
    "        npy_s_v.append([npy_mean, npy_std])\n",
    "        npy_s_k.append([len(npy_all)/16000/60])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [24:25, 122.15s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Function to process a single speaker\n",
    "def process_speaker(speaker):\n",
    "    if speaker in []:\n",
    "        print(\"Skip: \", speaker)\n",
    "        return\n",
    "    \n",
    "    ori_data_path_npy = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_npy_hips/\"\n",
    "    ori_data_path = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_v0.2.1_hips/\"\n",
    "    ori_data_path_ann = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_v0.2.1_hips/\"\n",
    "    cache_path = f\"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_{target_fps}_full_hips/\"\n",
    "    load_type = \"train\"\n",
    "\n",
    "    all_data = os.listdir(f\"{ori_data_path_npy}{speaker}\")\n",
    "    npy_all = []\n",
    "    json_all = []\n",
    "    bvh_all = []   \n",
    "\n",
    "    for ii, file in tqdm(enumerate(all_data)):\n",
    "        file = file[:-4]\n",
    "        shutil.copy(f\"{ori_data_path_npy}/{file.split('_')[0]}/{file}.npy\", f\"{cache_path}{load_type}/wave16k/{file}.npy\")\n",
    "        try:\n",
    "            shutil.copy(f\"{ori_data_path}/{file.split('_')[0]}/{file}.TextGrid\", f\"{cache_path}{load_type}/text/{file}.TextGrid\")\n",
    "        except:\n",
    "            print(f\"{file}.TextGrid\")\n",
    "        try:\n",
    "            shutil.copy(f\"{ori_data_path_ann}/{file.split('_')[0]}/{file}.txt\", f\"{cache_path}{load_type}/sem/{file}.txt\")\n",
    "        except:\n",
    "            print(f\"{file}.txt\")\n",
    "        try:\n",
    "            shutil.copy(f\"{ori_data_path_ann}/{file.split('_')[0]}/{file}.csv\", f\"{cache_path}{load_type}/emo/{file}.csv\")\n",
    "        except:\n",
    "            print(f\"{file}.csv\")\n",
    "\n",
    "        npy_all.extend(list(np.load(f\"{ori_data_path_npy}/{file.split('_')[0]}/{file}.npy\")))\n",
    "\n",
    "        with open(f\"{ori_data_path}/{file.split('_')[0]}/{file}.json\", \"r\", encoding='utf-8') as json_file_raw:\n",
    "            json_file = json.load(json_file_raw)\n",
    "            with open(f\"{cache_path}{load_type}/facial52/{file}.json\", \"w\") as reduced_json:\n",
    "                counter = 0\n",
    "                new_frames_list = []\n",
    "                for json_data in json_file[\"frames\"]:\n",
    "                    json_all.append(json_data[\"weights\"])\n",
    "                    if counter % reduce_factor_json == 0:\n",
    "                        new_frames_list.append(json_data)\n",
    "                    counter += 1\n",
    "                json_new = {\"names\": json_file[\"names\"], \"frames\": new_frames_list}\n",
    "                json.dump(json_new, reduced_json)\n",
    "\n",
    "            with open(f\"{ori_data_path}/{file.split('_')[0]}/{file}.bvh\", \"r\") as bvh_file:\n",
    "                with open(f\"{cache_path}{load_type}/bvh_rot/{file}.bvh\", \"w\") as reduced_raw_bvh:\n",
    "                    with open(f\"{cache_path}{load_type}/bvh_full/{file}.bvh\", \"w\") as reduced_full_bvh:\n",
    "                        with open(f\"{cache_path}{load_type}/bvh_rot_vis/{file}.bvh\", \"w\") as reduced_trainable_bvh:\n",
    "                            for i, line_data in enumerate(bvh_file.readlines()):\n",
    "                                if i < 431: \n",
    "                                    reduced_full_bvh.write(line_data)\n",
    "                                    reduced_trainable_bvh.write(line_data)\n",
    "                                if i >= 431:\n",
    "                                    data = np.fromstring(line_data, dtype=float, sep=' ')\n",
    "                                    bvh_all.append(data)\n",
    "                                    if i % reduce_factor_bvh == 0:\n",
    "                                        reduced_full_bvh.write(line_data)\n",
    "                                        trainable_rotation = np.zeros_like(data)\n",
    "                                        for k, v in target_list.items():\n",
    "                                            trainable_rotation[ori_list[k][1] - v:ori_list[k][1]] = data[ori_list[k][1] - v:ori_list[k][1]]\n",
    "\n",
    "                                        trainable_line_data = np.array2string(trainable_rotation, max_line_width=np.inf, precision=6, suppress_small=False, separator=' ')\n",
    "                                        reduced_trainable_bvh.write(trainable_line_data[1:-2] + \"\\n\")\n",
    "                                        data_rotation = np.zeros((1))   \n",
    "                                        for k, v in target_list.items():\n",
    "                                            data_rotation = np.concatenate((data_rotation, data[ori_list[k][1] - v:ori_list[k][1]]))                             \n",
    "                                            raw_line_data = np.array2string(data_rotation[1:], max_line_width=np.inf, precision=6, suppress_small=False, separator=' ')\n",
    "                                        reduced_raw_bvh.write(raw_line_data[1:-2] + \"\\n\")\n",
    "\n",
    "    npy_all = np.array(npy_all)\n",
    "    npy_mean = np.mean(npy_all, axis=0)\n",
    "    npy_std = np.std(npy_all, axis=0)\n",
    "    np.save(f\"{cache_path}{load_type}/wave16k/npy_mean_{speaker}.npy\", npy_mean)\n",
    "    np.save(f\"{cache_path}{load_type}/wave16k/npy_std_{speaker}.npy\", npy_std)  \n",
    "\n",
    "    json_all = np.array(json_all)\n",
    "    json_mean = np.mean(json_all, axis=0)\n",
    "    json_std = np.std(json_all, axis=0)\n",
    "    np.save(f\"{cache_path}{load_type}/facial52/json_mean_{speaker}.npy\", json_mean)\n",
    "    np.save(f\"{cache_path}{load_type}/facial52/json_std_{speaker}.npy\", json_std)\n",
    "\n",
    "    bvh_all = np.array(bvh_all)\n",
    "    bvh_mean = np.mean(bvh_all, axis=0)\n",
    "    bvh_std = np.std(bvh_all, axis=0)\n",
    "    data_rotation = np.zeros((1))\n",
    "    for k, v in target_list.items():\n",
    "        data_rotation = np.concatenate((data_rotation, bvh_mean[ori_list[k][1] - v:ori_list[k][1]]))\n",
    "    new_npy_mean = data_rotation[1:]\n",
    "    data_rotation = np.zeros((1))\n",
    "    for k, v in target_list.items():\n",
    "        data_rotation = np.concatenate((data_rotation, bvh_std[ori_list[k][1] - v:ori_list[k][1]]))\n",
    "    new_npy_std = data_rotation[1:]\n",
    "    np.save(f\"{cache_path}{load_type}/bvh_rot/bvh_mean_{speaker}.npy\", new_npy_mean)\n",
    "    np.save(f\"{cache_path}{load_type}/bvh_rot/bvh_std_{speaker}.npy\", new_npy_std)\n",
    "\n",
    "# Set up multiprocessing pool\n",
    "if __name__ == \"__main__\":\n",
    "    target_fps = 30\n",
    "    ori_list = joint_list[\"beat_joints\"]\n",
    "    target_list = joint_list[\"beat_full\"]\n",
    "    reduce_factor_json = int(60 / target_fps)\n",
    "    reduce_factor_bvh = int(120 / target_fps)\n",
    "    cache_path = f\"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_{target_fps}_full_hips/\"\n",
    "    \n",
    "    # Create required directories if they don't exist\n",
    "    load_type = \"train\"\n",
    "    if not os.path.exists(f\"{cache_path}{load_type}/\"):\n",
    "        os.makedirs(f\"{cache_path}{load_type}/wave16k/\")\n",
    "        os.makedirs(f\"{cache_path}{load_type}/bvh_rot/\")\n",
    "        os.makedirs(f\"{cache_path}{load_type}/bvh_full/\")\n",
    "        os.makedirs(f\"{cache_path}{load_type}/bvh_rot_vis/\")\n",
    "        os.makedirs(f\"{cache_path}{load_type}/facial52/\")\n",
    "        os.makedirs(f\"{cache_path}{load_type}/text/\")\n",
    "        os.makedirs(f\"{cache_path}{load_type}/emo/\")\n",
    "        os.makedirs(f\"{cache_path}{load_type}/sem/\")\n",
    "\n",
    "    # List of speakers to process\n",
    "    speakers = list(range(1, 2))\n",
    "\n",
    "    # Create a pool of workers equal to the number of CPUs\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        pool.map(process_speaker, speakers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = f\"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_{target_fps}_full_hips/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[239.66666666666666]]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npy_s_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18755737\n",
      " audio mean: -1.533552131149918e-05, std: 0.11750704050064087\n",
      "[7.05451693e-02 7.03765864e-02 5.55606167e-01 7.13724240e-01\n",
      " 7.13825507e-01 7.30097568e-03 1.25667172e-01 1.24597327e-01\n",
      " 2.09337054e-02 2.13082851e-02 6.43908944e-03 6.52360686e-03\n",
      " 3.30387944e-01 4.41611129e-03 7.39194859e-04 3.30778832e-01\n",
      " 2.63136211e-01 2.67653520e-01 6.34216116e-02 6.33497450e-02\n",
      " 1.76891711e-01 1.76920544e-01 5.83773975e-02 4.40580928e-03\n",
      " 1.82119109e-01 1.35550601e-02 1.35477064e-01 4.22890636e-02\n",
      " 4.03363728e-02 3.99690877e-02 4.43234250e-02 3.05983329e-01\n",
      " 1.90806897e-03 3.35903770e-01 3.38112221e-01 1.08650603e-02\n",
      " 1.24035169e-02 2.67550208e-02 1.69665602e-03 1.68684369e-01\n",
      " 1.15747320e-01 2.40894488e-02 7.65906285e-02 9.96368023e-01\n",
      " 9.72321205e-01 4.77366419e-02 6.15470697e-02 6.57291685e-02\n",
      " 6.86528965e-02 8.43922932e-02 9.35055864e-02]\n",
      " json mean: [2.74052114e-02 2.73530956e-02 3.92775515e-01 2.95098140e-01\n",
      " 2.95280231e-01 2.94794111e-02 2.34159969e-01 2.42457223e-01\n",
      " 2.52481095e-03 2.55462249e-03 4.98790740e-03 5.03254117e-03\n",
      " 2.91118648e-01 3.54982076e-03 4.45004649e-04 1.96339709e-01\n",
      " 2.08950228e-01 2.10650644e-01 2.83170992e-01 2.83183247e-01\n",
      " 6.51983640e-01 6.51982321e-01 1.77862071e-01 1.03925556e-02\n",
      " 2.62137705e-01 1.93777804e-02 1.55559702e-01 1.27150260e-01\n",
      " 1.24544930e-01 1.42082142e-02 1.66831679e-02 2.58524776e-01\n",
      " 8.73728082e-03 4.36100837e-01 4.34509008e-01 5.42195535e-02\n",
      " 5.58465126e-02 6.57626290e-02 4.78300485e-03 1.01152457e-01\n",
      " 8.06686390e-02 4.48671920e-02 2.39031481e-01 3.51835923e-01\n",
      " 3.22233592e-01 2.97159891e-01 3.17563124e-01 7.95865080e-02\n",
      " 8.27592628e-02 1.23869472e-01 1.36267915e-01], std: [0.07206601 0.07197985 0.20224619 0.22922503 0.22924129 0.02318394\n",
      " 0.09618509 0.09577478 0.03925725 0.03960692 0.02177254 0.02191496\n",
      " 0.15595845 0.01803088 0.00737694 0.15605068 0.1391833  0.14037291\n",
      " 0.06833064 0.06829191 0.11411706 0.11412636 0.06555702 0.01800983\n",
      " 0.11579094 0.03158985 0.09986877 0.055797   0.05449357 0.0542449\n",
      " 0.05712332 0.15008791 0.01185206 0.15725493 0.15777103 0.02828218\n",
      " 0.03021825 0.04438124 0.01117619 0.11143824 0.09231075 0.04211243\n",
      " 0.07509044 0.2708362  0.26754799 0.05928199 0.06731324 0.06956261\n",
      " 0.0710929  0.07882214 0.08296895]\n",
      "\n",
      "\n",
      "[-2.18360142e+01  6.28020384e+00  3.39813208e+00  1.83486021e+01\n",
      " -4.60460696e-01  1.16126505e+00  8.56711943e-01  1.41249473e-01\n",
      " -2.54447634e-01  2.60052247e+00  6.29272690e-01 -4.27033485e-01\n",
      "  6.71145961e+00  9.08213876e-01  1.38852792e+00  1.33284003e+01\n",
      " -2.43160372e-01  1.02385428e+01 -1.81166587e+00  3.34014680e+00\n",
      " -1.52462305e+01 -2.11792595e+00  8.46856598e-01 -3.51673944e+00\n",
      " -1.75490798e-07 -1.12576687e-07 -3.26175869e-09 -3.26101227e-06\n",
      " -9.06429950e+00  1.42951673e+01  8.74820839e+00  1.40161025e+01\n",
      "  5.94313006e+01 -3.59127053e+02  4.21636461e+01  3.67464939e+02\n",
      " -1.69128028e+00 -1.10998069e+01 -1.20831104e+01  4.18893437e+00\n",
      " -6.87832860e+00  3.24781073e+01  4.70030675e-07  3.93803681e-07\n",
      "  3.56099288e+01  3.50817996e-07  8.05623722e-08  2.16000474e+01\n",
      "  4.06257669e-07 -1.03374233e-08 -4.62781186e-08 -6.50910103e+00\n",
      " -1.99889667e+00 -2.04373665e+00  1.22278672e+00 -6.98139954e+00\n",
      "  3.80333833e+01 -3.46887578e+00 -1.50427711e+00  4.32804647e+01\n",
      " -1.70393786e+00 -3.05835889e-01  1.92927059e+01 -1.21564417e-07\n",
      " -3.99795501e-07 -3.04703476e-09 -4.30600068e+00 -5.05700802e+00\n",
      " -3.13929349e+00 -6.22308060e+00 -1.76602665e+00  2.79108635e+01\n",
      " -8.93486141e+00 -3.87154150e+00  4.47228967e+01 -6.72621362e+00\n",
      " -2.00520674e+00  3.18010754e+01  8.21472393e-08 -1.21267894e-08\n",
      "  7.82208589e-09 -6.41395870e-01  7.66016642e+00 -1.92627129e-01\n",
      "  6.48559329e+00 -9.54512661e+00  2.28795486e+01  2.26388462e+00\n",
      "  6.78623582e-01  2.66756112e+01  1.88081107e+00  3.94925899e-01\n",
      "  2.14804215e+01  5.55725971e-08  2.99141104e-07  9.61145194e-09\n",
      "  4.91456130e+01  1.01903430e+01 -1.23994063e-01  1.47966258e+00\n",
      " -4.63114447e-01  4.03526367e+00  4.13780070e+00 -6.27771712e+00\n",
      "  1.09587591e+01 -1.47803681e-06 -1.65586912e-06  1.93703476e-06\n",
      " -1.76476483e-06  5.29961387e+00 -1.65322638e+01  1.57331243e+01\n",
      " -1.67498546e+01 -5.30774155e+01 -2.41021189e+02 -1.07131335e+02\n",
      " -2.49923813e+02 -6.86689951e-01  1.73744496e+01  1.24670239e+00\n",
      "  2.83284385e-01  1.24551589e+00 -2.84434243e+01  3.97781186e-07\n",
      " -3.99427403e-07 -2.26851747e+01  2.48650307e-07 -2.42351738e-07\n",
      " -1.90676473e+01  1.76257669e-07 -4.08077710e-08 -1.62781186e-08\n",
      " -1.32209157e+01  7.41373758e-01 -1.10106577e-01 -7.64451759e-01\n",
      "  2.98922497e+00 -2.20570623e+01 -2.69005724e+00  8.91585988e-01\n",
      " -3.20754165e+01 -1.01814122e+00  1.24801861e-01 -1.14007748e+01\n",
      " -4.00296524e-07  1.01411043e-07 -3.35991820e-08 -7.37653544e+00\n",
      "  8.61291760e-01  9.30840449e-01 -4.37483094e-01  9.39669027e+00\n",
      " -6.79341935e+00 -5.87097929e+00  1.56304047e+00 -2.74839753e+01\n",
      " -4.43188391e+00  8.72107996e-01 -2.03304175e+01  6.42126789e-07\n",
      " -1.29795501e-07  3.34458078e-08 -5.96353878e+00 -7.36640525e+00\n",
      "  1.23563981e+00  5.29381881e+00  3.84506292e+00 -2.86848325e+01\n",
      "  1.64904229e+00 -3.98929709e-01 -1.89773425e+01  1.32896208e+00\n",
      " -2.07415094e-01 -1.49786930e+01 -7.86912065e-08  2.36605317e-08\n",
      "  1.08895706e-08  4.84743853e+01 -5.59653847e+00 -6.43020910e+00\n",
      "  5.88591375e-01  5.51390800e+00 -2.75066936e+00  3.76555798e+00\n",
      "  5.59712249e+00 -9.85123668e+00  7.66605317e-07  5.68302658e-08\n",
      "  1.11303681e-06  2.43078288e+01 -5.57335274e+00 -3.86250065e+00\n",
      "  8.17274825e+00  1.80368098e-08  7.51656442e-07 -6.26448030e+00\n",
      " -1.73594390e+01 -1.20447103e+01  3.03680982e-09  7.76687117e-08\n",
      "  3.05449898e-07 -1.45643709e+01 -4.79550102e-09  5.21165644e-08\n",
      "  1.58087935e-07  1.96768916e-07  3.84151329e-08  2.69208457e+01\n",
      " -1.09605739e+00 -7.90838115e-01  9.26301528e-01 -1.02862986e-08\n",
      "  6.24233129e-08 -5.30278202e+00  1.85981621e+01  1.69215638e+01\n",
      "  3.03067485e-08  1.44274029e-08  1.32862986e-07  2.65292057e-01\n",
      "  1.84049080e-09 -9.42740286e-09 -1.79141104e-08 -4.81595092e-09\n",
      "  0.00000000e+00] (225,)\n",
      "[2.90664417e+00 6.03280672e+00 2.97985285e+00 3.31704194e+00\n",
      " 5.24907331e-01 2.63303158e+00 4.49936094e-01 1.26936967e-01\n",
      " 4.52464774e-01 8.23432876e-01 1.42524123e+00 7.51360361e-01\n",
      " 8.29701472e-01 3.30880088e-01 1.03564152e+00 2.87993661e+00\n",
      " 1.71353383e+00 2.03950710e+00 1.38093596e+00 2.61227134e+00\n",
      " 2.09400911e+00 5.58526670e-01 2.35131107e-01 5.86973587e-01\n",
      " 9.88973210e-07 9.33759748e-07 5.05086191e-07 8.96209581e-07\n",
      " 1.13918840e+00 1.45614678e+00 1.12125852e+01 1.07819110e+01\n",
      " 3.91982155e+00 1.76340386e+02 1.41178558e+01 1.75301204e+02\n",
      " 3.47805402e+00 6.98718772e+00 1.41041312e+01 1.24852669e+00\n",
      " 1.98533435e+00 9.66951031e+00 1.50830747e-06 1.44270623e-06\n",
      " 1.29791413e+01 1.40279969e-06 1.52838932e-06 5.71758353e+00\n",
      " 1.34340231e-06 1.68883786e-06 1.60372313e-06 2.66368227e+00\n",
      " 5.44207597e-01 4.88717299e-01 7.67492844e-01 1.40570088e+00\n",
      " 1.11991439e+01 8.89399262e-01 6.91538710e-01 1.26649507e+01\n",
      " 3.91088822e-01 1.14588936e-01 4.52678732e+00 1.62797540e-06\n",
      " 1.78692134e-06 1.69555262e-06 7.09668019e-01 6.84912685e-01\n",
      " 4.82070446e-01 2.66336969e+00 2.89939639e+00 8.55312782e+00\n",
      " 1.66467352e+00 1.40180496e+00 9.74869579e+00 1.29615824e+00\n",
      " 7.26446298e-01 6.63070622e+00 1.73618423e-06 1.89456120e-06\n",
      " 1.46187758e-06 1.49942363e+00 4.01526048e-01 6.11231720e-01\n",
      " 3.21637388e+00 2.08036715e+00 9.83091682e+00 1.09091578e+00\n",
      " 4.94250817e-01 1.34275702e+01 5.88811626e-01 2.10859091e-01\n",
      " 6.95192109e+00 1.46220609e-06 1.83754951e-06 1.69585373e-06\n",
      " 6.30984645e+00 2.51717343e+00 3.76622897e+00 2.58768706e+00\n",
      " 2.96864749e+00 6.74119816e+00 2.09870667e+00 2.85502271e+00\n",
      " 5.22079794e+00 2.29932155e-06 2.20835449e-06 1.85217017e-06\n",
      " 8.30306443e-07 1.02901150e+00 1.13948214e+00 1.01546665e+01\n",
      " 1.04523370e+01 3.82538262e+00 1.07744885e+02 4.46611731e+01\n",
      " 1.06743640e+02 2.76492009e+00 6.46270344e+00 1.00615726e+01\n",
      " 7.93049064e-01 2.21429231e+00 9.42018775e+00 1.29861543e-06\n",
      " 1.11801006e-06 1.28243624e+01 1.17871079e-06 1.16022959e-06\n",
      " 6.92383286e+00 1.06681526e-06 1.36632797e-06 1.60030699e-06\n",
      " 2.09885705e+00 6.82189914e-01 4.15960857e-01 5.87440527e-01\n",
      " 1.64638649e+00 7.10621652e+00 9.59334924e-01 5.21371080e-01\n",
      " 1.22207008e+01 4.80999537e-01 8.43155071e-02 5.42327966e+00\n",
      " 1.52534079e-06 1.71245947e-06 1.61919592e-06 1.31529459e+00\n",
      " 8.16632062e-01 7.28117290e-01 4.98504323e-01 1.80057907e+00\n",
      " 7.05336754e+00 1.61282708e+00 7.73327780e-01 8.00007363e+00\n",
      " 1.32591339e+00 4.52669601e-01 6.26496876e+00 1.92731532e-06\n",
      " 1.87040883e-06 1.61525068e-06 1.86147522e+00 5.64877449e-01\n",
      " 5.96860656e-01 2.33350831e+00 1.86666476e+00 9.67099892e+00\n",
      " 1.07411246e+00 3.06533750e-01 1.25289193e+01 5.63632261e-01\n",
      " 1.35273796e-01 6.44319099e+00 1.16767292e-06 1.52033675e-06\n",
      " 1.49089741e-06 5.41207874e+00 3.12085323e+00 4.36685433e+00\n",
      " 1.84360439e+00 2.38546024e+00 4.31125912e+00 2.60253401e+00\n",
      " 3.82613332e+00 6.77080734e+00 2.36051477e-06 2.33023195e-06\n",
      " 2.06753407e-06 1.27727008e+01 7.16320775e+00 1.05127035e+01\n",
      " 1.70353577e+01 9.43908373e-07 1.64598486e-06 6.28368809e+00\n",
      " 8.72134836e+00 1.53092385e+01 5.21835071e-07 1.13596183e-06\n",
      " 1.20966750e-06 2.90914808e+00 1.30816514e-06 1.45136337e-06\n",
      " 8.14646111e-07 1.07887969e-06 1.17172148e-06 7.63508255e+00\n",
      " 7.23814132e+00 9.67428074e+00 9.44200791e+00 1.04231363e-06\n",
      " 1.03044606e-06 5.65662159e+00 6.68525923e+00 1.20691881e+01\n",
      " 5.88594482e-07 1.38034199e-06 1.04990787e-06 6.03394305e+00\n",
      " 1.58069795e-06 1.13224404e-06 5.81502659e-07 1.30470570e-06\n",
      " 0.00000000e+00] (225,)\n"
     ]
    }
   ],
   "source": [
    "npy_s_v = []\n",
    "json_s_v = []\n",
    "bvh_s_v = []\n",
    "\n",
    "npy_path = f\"{cache_path}{load_type}/wave16k/\"\n",
    "bvh_path = f\"{cache_path}{load_type}/bvh_rot/\"\n",
    "json_path = f\"{cache_path}{load_type}/facial52/\" \n",
    "        \n",
    "for i in [1]: #range(1, 31):\n",
    "    npy_s_v.append([np.load(f\"{npy_path}npy_mean_{i}.npy\"), np.load(f\"{npy_path}npy_std_{i}.npy\")])\n",
    "    json_s_v.append([np.load(f\"{json_path}json_mean_{i}.npy\"), np.load(f\"{json_path}json_std_{i}.npy\")])\n",
    "    bvh_s_v.append([np.load(f\"{bvh_path}bvh_mean_{i}.npy\"), np.load(f\"{bvh_path}bvh_std_{i}.npy\")])\n",
    "\n",
    "all_length = 0\n",
    "new_m = np.zeros_like(npy_s_v[0][0])\n",
    "new_s = np.zeros_like(npy_s_v[0][0])\n",
    "for i, (m, s) in enumerate(npy_s_v):\n",
    "    all_length += npy_s_k[i][0]\n",
    "    new_m += npy_s_k[i][0] * m\n",
    "new_m /= all_length\n",
    "for i, (m, s) in enumerate(npy_s_v):\n",
    "    new_s += ((s**2) + (m-new_m)**2) * npy_s_k[i][0]\n",
    "\n",
    "print(new_s)\n",
    "new_s /= all_length\n",
    "new_s = np.sqrt(new_s)\n",
    "print(f\" audio mean: {new_m}, std: {new_s}\") \n",
    "np.save(f\"{npy_path}npy_mean.npy\", new_m)\n",
    "np.save(f\"{npy_path}/npy_std.npy\", new_s)  \n",
    "\n",
    "new_m = np.zeros_like(json_s_v[0][0])\n",
    "new_s = np.zeros_like(json_s_v[0][0])\n",
    "all_length = 0\n",
    "for i, (m, s) in enumerate(json_s_v):\n",
    "    all_length += npy_s_k[i][0]\n",
    "    new_m += npy_s_k[i][0] * m\n",
    "new_m /= all_length\n",
    "for i, (m, s) in enumerate(json_s_v):\n",
    "    new_s += ((s**2) + (m-new_m)**2) * npy_s_k[i][0]\n",
    "\n",
    "print(new_s)\n",
    "new_s /= all_length\n",
    "new_s = np.sqrt(new_s)\n",
    "print(f\" json mean: {new_m}, std: {new_s}\") \n",
    "np.save(f\"{json_path}json_mean.npy\", new_m)\n",
    "np.save(f\"{json_path}/json_std.npy\", new_s)\n",
    "\n",
    "new_m = np.zeros_like(bvh_s_v[0][0])\n",
    "new_s = np.zeros_like(bvh_s_v[0][0])\n",
    "all_length = 0\n",
    "for i, (m, s) in enumerate(bvh_s_v):\n",
    "    all_length += npy_s_k[i][0]\n",
    "    new_m += npy_s_k[i][0] * m\n",
    "new_m /= all_length\n",
    "for i, (m, s) in enumerate(bvh_s_v):\n",
    "    new_s += ((s**2) + (m-new_m)**2) * npy_s_k[i][0]\n",
    "new_s /= all_length\n",
    "new_s = np.sqrt(new_s)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(new_m, new_m.shape)\n",
    "print(new_s, new_s.shape)\n",
    "np.save(f\"{bvh_path}bvh_mean.npy\", new_m)\n",
    "np.save(f\"{bvh_path}/bvh_std.npy\", new_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_rule_english = {\n",
    "    # 4h speakers x 10\n",
    "    \"1, 2, 3, 4, 6, 7, 8, 9, 11\":{\n",
    "        # 48+40+100=188mins each\n",
    "        \"train\": [\n",
    "            \"0_9_9\", \"0_10_10\", \"0_11_11\", \"0_12_12\", \"0_13_13\", \"0_14_14\", \"0_15_15\", \"0_16_16\", \\\n",
    "            \"0_17_17\", \"0_18_18\", \"0_19_19\", \"0_20_20\", \"0_21_21\", \"0_22_22\", \"0_23_23\", \"0_24_24\", \\\n",
    "            \"0_25_25\", \"0_26_26\", \"0_27_27\", \"0_28_28\", \"0_29_29\", \"0_30_30\", \"0_31_31\", \"0_32_32\", \\\n",
    "            \"0_33_33\", \"0_34_34\", \"0_35_35\", \"0_36_36\", \"0_37_37\", \"0_38_38\", \"0_39_39\", \"0_40_40\", \\\n",
    "            \"0_41_41\", \"0_42_42\", \"0_43_43\", \"0_44_44\", \"0_45_45\", \"0_46_46\", \"0_47_47\", \"0_48_48\", \\\n",
    "            \"0_49_49\", \"0_50_50\", \"0_51_51\", \"0_52_52\", \"0_53_53\", \"0_54_54\", \"0_55_55\", \"0_56_56\", \\\n",
    "            \n",
    "            \"0_66_66\", \"0_67_67\", \"0_68_68\", \"0_69_69\", \"0_70_70\", \"0_71_71\",  \\\n",
    "            \"0_74_74\", \"0_75_75\", \"0_76_76\", \"0_77_77\", \"0_78_78\", \"0_79_79\",  \\\n",
    "            \"0_82_82\", \"0_83_83\", \"0_84_84\", \"0_85_85\",  \\\n",
    "            \"0_88_88\", \"0_89_89\", \"0_90_90\", \"0_91_91\", \"0_92_92\", \"0_93_93\",  \\\n",
    "            \"0_96_96\", \"0_97_97\", \"0_98_98\", \"0_99_99\", \"0_100_100\", \"0_101_101\",  \\\n",
    "            \"0_104_104\", \"0_105_105\", \"0_106_106\", \"0_107_107\", \"0_108_108\", \"0_109_109\",  \\\n",
    "            \"0_112_112\", \"0_113_113\", \"0_114_114\", \"0_115_115\", \"0_116_116\", \"0_117_117\",  \\\n",
    "            \n",
    "            \"1_2_2\", \"1_3_3\", \"1_4_4\", \"1_5_5\", \"1_6_6\", \"1_7_7\", \"1_8_8\", \"1_9_9\", \"1_10_10\", \"1_11_11\",\n",
    "        ],\n",
    "        # 8+7+10=25mins each\n",
    "        \"val\": [\n",
    "            \"0_57_57\", \"0_58_58\", \"0_59_59\", \"0_60_60\", \"0_61_61\", \"0_62_62\", \"0_63_63\", \"0_64_64\", \\\n",
    "            \"0_72_72\", \"0_80_80\", \"0_86_86\", \"0_94_94\", \"0_102_102\", \"0_110_110\", \"0_118_118\", \\\n",
    "            \"1_12_12\",\n",
    "        ],\n",
    "        # 8+7+10=25mins each\n",
    "        \"test\": [\n",
    "           \"0_1_1\", \"0_2_2\", \"0_3_3\", \"0_4_4\", \"0_5_5\", \"0_6_6\", \"0_7_7\", \"0_8_8\", \\\n",
    "           \"0_65_65\", \"0_73_73\", \"0_81_81\", \"0_87_87\", \"0_95_95\", \"0_103_103\", \"0_111_111\", \\\n",
    "           \"1_1_1\",\n",
    "        ],\n",
    "    },\n",
    "    \n",
    "    # 1h speakers x 20\n",
    "    \"5, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30\":{\n",
    "        # 8+7+20=35mins each\n",
    "        \"train\": [\n",
    "            \"0_9_9\", \"0_10_10\", \"0_11_11\", \"0_12_12\", \"0_13_13\", \"0_14_14\", \"0_15_15\", \"0_16_16\", \\\n",
    "            \"0_66_66\", \"0_74_74\", \"0_82_82\", \"0_88_88\", \"0_96_96\", \"0_104_104\", \"0_112_112\", \"0_118_118\", \\\n",
    "            \"1_2_2\", \"1_3_3\", \n",
    "            \"1_0_0\", \"1_4_4\", # for speaker 29 only\n",
    "        ],\n",
    "        # 4+3.5+5 = 12.5mins each\n",
    "        # 0_65_a and 0_65_b denote the frist and second half of sequence 0_65_65\n",
    "        \"val\": [\n",
    "            \"0_5_5\", \"0_6_6\", \"0_7_7\", \"0_8_8\",  \\\n",
    "            \"0_65_b\", \"0_73_b\", \"0_81_b\", \"0_87_b\", \"0_95_b\", \"0_103_b\", \"0_111_b\", \\\n",
    "            \"1_1_b\",\n",
    "        ],\n",
    "        # 4+3.5+5 = 12.5mins each\n",
    "        \"test\": [\n",
    "           \"0_1_1\", \"0_2_2\", \"0_3_3\", \"0_4_4\", \\\n",
    "           \"0_65_a\", \"0_73_a\", \"0_81_a\", \"0_87_a\", \"0_95_a\", \"0_103_a\", \"0_111_a\", \\\n",
    "           \"1_1_a\",\n",
    "        ],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_rule_english = {\n",
    "    # 4h speakers x 10\n",
    "    \"1, 2, 3, 4, 6, 7, 8, 9, 11\":{\n",
    "        # 48+40+100=188mins each\n",
    "        # 80\n",
    "        \"train\": [\n",
    "            \"0_9_9\", \"0_10_10\", \"0_11_11\", \"0_12_12\", \"0_3_3\", \"0_4_4\", \"0_5_5\", \"0_6_6\",\n",
    "        ],\n",
    "        # 8+7+10=25mins each\n",
    "        # 25\n",
    "        \"val\": [\n",
    "            \"0_7_7\", \"0_8_8\",\n",
    "        ],\n",
    "        # 8+7+10=25mins each\n",
    "        # 25\n",
    "        \"test\": [\n",
    "           \"0_1_1\", \"0_2_2\",\n",
    "        ],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_sequence(source_path, save_path_a, save_path_b, file_id, fps = 15, sr = 16000, tmp=\"/data/nas07/PersonalData/danh/PantoMatrix/beat_tmp7/\"):\n",
    "    if not os.path.exists(tmp): os.mkdir(tmp)\n",
    "    cut_point = 30 if file_id.split(\"_\")[0] == \"0\" else 300 #in seconds\n",
    "    if source_path.endswith(\".npy\"):\n",
    "        data = np.load(source_path)\n",
    "        data_a = data[:sr*cut_point]\n",
    "        data_b = data[sr*cut_point:]\n",
    "        np.save(save_path_a, data_a)\n",
    "        np.save(save_path_b, data_b)\n",
    "        \n",
    "    elif source_path.endswith(\".bvh\"):\n",
    "        copy_lines = 431 if \"full\" in source_path or \"vis\" in source_path else 0\n",
    "        with open(source_path, \"r\") as data:\n",
    "            with open(save_path_a, \"w\") as data_a:\n",
    "                with open(save_path_b, \"w\") as data_b:\n",
    "                    for i, line_data in enumerate(data.readlines()):\n",
    "                        if i < copy_lines:\n",
    "                            data_a.write(line_data)\n",
    "                            data_b.write(line_data)\n",
    "                        elif i < cut_point * fps:\n",
    "                            data_a.write(line_data)\n",
    "                        else:\n",
    "                            data_b.write(line_data)\n",
    "    \n",
    "    elif source_path.endswith(\".json\"):\n",
    "        with open(source_path, \"r\", encoding='utf-8') as data:\n",
    "            json_file = json.load(data)\n",
    "            with open(save_path_a, \"w\") as data_a:\n",
    "                with open(save_path_b, \"w\") as data_b:\n",
    "                    new_frames_a = []\n",
    "                    new_frames_b = []\n",
    "                    for json_data in json_file[\"frames\"]:\n",
    "                        if json_data[\"time\"] < cut_point:\n",
    "                            new_frames_a.append(json_data)\n",
    "                        else:\n",
    "                            new_frame = json_data.copy()\n",
    "                            new_frame[\"time\"]-=cut_point\n",
    "                            new_frames_b.append(new_frame)\n",
    "                    json_new_a = {\"names\":json_file[\"names\"], \"frames\": new_frames_a}\n",
    "                    json_new_b = {\"names\":json_file[\"names\"], \"frames\": new_frames_b}\n",
    "                    json.dump(json_new_a, data_a)\n",
    "                    json.dump(json_new_b, data_b) \n",
    "        \n",
    "    else:\n",
    "        # processing in the dataloader\n",
    "        shutil.copy(source_path, save_path_a)\n",
    "        shutil.copy(source_path, save_path_b)\n",
    "    try:\n",
    "        shutil.move(source_path, tmp)\n",
    "    except:\n",
    "        print(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_wayne_0_32_32.csv',\n",
       " '1_wayne_0_41_41.csv',\n",
       " '1_wayne_0_51_51.csv',\n",
       " '1_wayne_0_22_22.csv',\n",
       " '1_wayne_0_12_12.csv',\n",
       " '1_wayne_0_97_97.csv',\n",
       " '1_wayne_1_6_6.csv',\n",
       " '1_wayne_0_71_71.csv',\n",
       " '1_wayne_0_116_116.csv',\n",
       " '1_wayne_0_84_84.csv',\n",
       " '1_wayne_0_11_11.csv',\n",
       " '1_wayne_0_31_31.csv',\n",
       " '1_wayne_0_107_107.csv',\n",
       " '1_wayne_0_42_42.csv',\n",
       " '1_wayne_1_3_3.csv',\n",
       " '1_wayne_1_11_11.csv',\n",
       " '1_wayne_0_52_52.csv',\n",
       " '1_wayne_0_21_21.csv',\n",
       " '1_wayne_0_82_82.csv',\n",
       " '1_wayne_0_106_106.csv',\n",
       " '1_wayne_0_17_17.csv',\n",
       " '1_wayne_0_74_74.csv',\n",
       " '1_wayne_0_92_92.csv',\n",
       " '1_wayne_0_44_44.csv',\n",
       " '1_wayne_0_37_37.csv',\n",
       " '1_wayne_0_117_117.csv',\n",
       " '1_wayne_0_27_27.csv',\n",
       " '1_wayne_0_54_54.csv',\n",
       " '1_wayne_0_47_47.csv',\n",
       " '1_wayne_0_34_34.csv',\n",
       " '1_wayne_0_24_24.csv',\n",
       " '1_wayne_1_9_9.csv',\n",
       " '1_wayne_0_14_14.csv',\n",
       " '1_wayne_0_67_67.csv',\n",
       " '1_wayne_0_91_91.csv',\n",
       " '1_wayne_0_77_77.csv',\n",
       " '1_wayne_0_78_78.csv',\n",
       " '1_wayne_0_68_68.csv',\n",
       " '1_wayne_1_8_8.csv',\n",
       " '1_wayne_0_48_48.csv',\n",
       " '1_wayne_0_104_104.csv',\n",
       " '1_wayne_0_9_9.csv',\n",
       " '1_wayne_0_28_28.csv',\n",
       " '1_wayne_0_38_38.csv',\n",
       " '1_wayne_0_115_115.csv',\n",
       " '1_wayne_0_18_18.csv',\n",
       " '1_wayne_0_114_114.csv',\n",
       " '1_wayne_1_2_2.csv',\n",
       " '1_wayne_0_105_105.csv',\n",
       " '1_wayne_1_7_7.csv',\n",
       " '1_wayne_0_98_98.csv',\n",
       " '1_wayne_0_88_88.csv',\n",
       " '1_wayne_0_23_23.csv',\n",
       " '1_wayne_0_50_50.csv',\n",
       " '1_wayne_0_40_40.csv',\n",
       " '1_wayne_0_33_33.csv',\n",
       " '1_wayne_0_96_96.csv',\n",
       " '1_wayne_0_101_101.csv',\n",
       " '1_wayne_0_70_70.csv',\n",
       " '1_wayne_0_13_13.csv',\n",
       " '1_wayne_1_5_5.csv',\n",
       " '1_wayne_0_85_85.csv',\n",
       " '1_wayne_0_10_10.csv',\n",
       " '1_wayne_0_20_20.csv',\n",
       " '1_wayne_0_53_53.csv',\n",
       " '1_wayne_1_10_10.csv',\n",
       " '1_wayne_0_43_43.csv',\n",
       " '1_wayne_0_30_30.csv',\n",
       " '1_wayne_0_75_75.csv',\n",
       " '1_wayne_0_93_93.csv',\n",
       " '1_wayne_0_83_83.csv',\n",
       " '1_wayne_0_16_16.csv',\n",
       " '1_wayne_0_55_55.csv',\n",
       " '1_wayne_0_26_26.csv',\n",
       " '1_wayne_0_36_36.csv',\n",
       " '1_wayne_0_45_45.csv',\n",
       " '1_wayne_0_56_56.csv',\n",
       " '1_wayne_0_25_25.csv',\n",
       " '1_wayne_0_100_100.csv',\n",
       " '1_wayne_0_35_35.csv',\n",
       " '1_wayne_0_46_46.csv',\n",
       " '1_wayne_0_90_90.csv',\n",
       " '1_wayne_0_76_76.csv',\n",
       " '1_wayne_0_66_66.csv',\n",
       " '1_wayne_0_15_15.csv',\n",
       " '1_wayne_0_69_69.csv',\n",
       " '1_wayne_0_79_79.csv',\n",
       " '1_wayne_0_49_49.csv',\n",
       " '1_wayne_0_113_113.csv',\n",
       " '1_wayne_0_39_39.csv',\n",
       " '1_wayne_0_29_29.csv',\n",
       " '1_wayne_0_109_109.csv',\n",
       " '1_wayne_0_19_19.csv',\n",
       " '1_wayne_0_108_108.csv',\n",
       " '1_wayne_0_89_89.csv',\n",
       " '1_wayne_0_112_112.csv',\n",
       " '1_wayne_1_4_4.csv',\n",
       " '1_wayne_0_99_99.csv']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(default_path+\"/\"+folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mroot\u001b[49m\u001b[38;5;241m/\u001b[39mnguyen\u001b[38;5;241m/\u001b[39mresearch\u001b[38;5;241m/\u001b[39mPantoMatrix\u001b[38;5;241m/\u001b[39mbeat_4english_30_full\u001b[38;5;241m/\u001b[39mtrain_copy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'root' is not defined"
     ]
    }
   ],
   "source": [
    "/root/nguyen/research/PantoMatrix/beat_4english_30_full/train_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_30_full9/train_copy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bvh_rot_vis',\n",
       " 'text',\n",
       " 'bvh_full',\n",
       " 'emo',\n",
       " 'bvh_rot',\n",
       " 'wave16k',\n",
       " 'facial52',\n",
       " 'sem']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = os.listdir(default_path)\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'npy'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(default_path+\"/\"+folders[5])[111].split(\".\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_30_full9/train_copy'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_30_full9/train_copy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bvh_rot_vis',\n",
       " 'text',\n",
       " 'bvh_full',\n",
       " 'emo',\n",
       " 'bvh_rot',\n",
       " 'wave16k',\n",
       " 'facial52',\n",
       " 'sem']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(default_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1/1 [00:00<00:00,  5.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# spilt data\n",
    "speaker_names = [\n",
    "    \"wayne\", \"scott\", \"solomon\", \"lawrence\", \"stewart\", \"carla\", \"sophie\", \"catherine\", \"miranda\", \"kieks\", \\\n",
    "    \"nidal\", \"zhao\", \"lu\", \"zhang\", \"carlos\", \"jorge\", \"itoi\", \"daiki\", \"jaime\", \"li\", \\\n",
    "    \"ayana\", \"luqi\", \"hailing\", \"kexin\", \"goto\", \"reamey\", \"yingqing\", \"tiffnay\", \"hanieh\", \"katya\",\n",
    "]\n",
    "default_path = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_30_full9/train_copy\"\n",
    "four_hour_speakers = \"1, 2, 3, 4, 6, 7, 8, 9, 11\".split(\", \")\n",
    "one_hour_speakers = \"5, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30\".split(\", \")\n",
    "folders = os.listdir(default_path)\n",
    "if not os.path.exists(default_path.replace(\"train\", \"val\")): os.mkdir(default_path.replace(\"train\", \"val\"))\n",
    "if not os.path.exists(default_path.replace(\"train\", \"test\")): os.mkdir(default_path.replace(\"train\", \"test\"))\n",
    "endwith = []\n",
    "for folder in folders:\n",
    "    # print(os.listdir(default_path+\"/\"+folder))\n",
    "    # print(\"\\n\\n\")\n",
    "    if not os.path.exists(default_path.replace(\"train\", \"val\")+\"/\"+folder): os.mkdir(default_path.replace(\"train\", \"val\")+\"/\"+folder)\n",
    "    if not os.path.exists(default_path.replace(\"train\", \"test\")+\"/\"+folder): os.mkdir(default_path.replace(\"train\", \"test\")+\"/\"+folder)\n",
    "    endwith.append(os.listdir(default_path+\"/\"+folder)[50].split(\".\")[-1])\n",
    "    \n",
    "for speaker_id in tqdm([1]):\n",
    "    val = split_rule_english[\"5, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30\"][\"val\"] if str(speaker_id) in one_hour_speakers else split_rule_english[\"1, 2, 3, 4, 6, 7, 8, 9, 11\"][\"val\"]\n",
    "    test = split_rule_english[\"5, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30\"][\"test\"] if str(speaker_id) in one_hour_speakers else split_rule_english[\"1, 2, 3, 4, 6, 7, 8, 9, 11\"][\"test\"]\n",
    "    for file_id in val:\n",
    "        for ide, folder in enumerate(folders):\n",
    "            if \"b\" in file_id:\n",
    "                print(f\"DATA: {default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id.split('_')[0]}_{file_id.split('_')[1]}_{file_id.split('_')[1]}.{endwith[ide]}\")\n",
    "                cut_sequence(\n",
    "                    source_path=f\"{default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id.split('_')[0]}_{file_id.split('_')[1]}_{file_id.split('_')[1]}.{endwith[ide]}\",\n",
    "                    save_path_a=f\"{default_path.replace('train', 'test')}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id.split('_')[0]}_{file_id.split('_')[1]}_a.{endwith[ide]}\",\n",
    "                    save_path_b=f\"{default_path.replace('train', 'val')}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id.split('_')[0]}_{file_id.split('_')[1]}_b.{endwith[ide]}\",\n",
    "                    file_id = file_id,\n",
    "                        )\n",
    "            else:\n",
    "                #pass\n",
    "                try:\n",
    "                    shutil.move(f\"{default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id}.{endwith[ide]}\", f\"{default_path.replace('train', 'val')}/{folder}/\")\n",
    "                except:\n",
    "                    print(f\"{default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id}.{endwith[ide]}\")\n",
    "    for file_id in test:\n",
    "        for ide, folder in enumerate(folders):\n",
    "            if \"a\" in file_id:\n",
    "                pass\n",
    "            else:\n",
    "                #pass\n",
    "                try:\n",
    "                    shutil.move(f\"{default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id}.{endwith[ide]}\", f\"{default_path.replace('train', 'test')}/{folder}/\")\n",
    "                except:\n",
    "                    print(f\"{default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id}.{endwith[ide]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'21_ayana_0_93_93.npy'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_data_path_npy = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_npy/\"\n",
    "all_data = os.listdir(f\"{ori_data_path_npy}{21}\")\n",
    "all_data[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_data_path_npy = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_npy/\"\n",
    "ori_data_path = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_v0.2.1/\"\n",
    "ori_data_path_ann = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_v0.2.1/\"\n",
    "cache_path = f\"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_30_full9/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123it [9:42:38, 284.22s/it]\n"
     ]
    }
   ],
   "source": [
    "npy_all = []\n",
    "for ii, file in tqdm(enumerate(all_data)):\n",
    "    file = file[:-4]\n",
    "    shutil.copy(f\"{ori_data_path_npy}/{file.split('_')[0]}/{file}.npy\", f\"{cache_path}{load_type}/wave16k/{file}.npy\")\n",
    "    try:\n",
    "        shutil.copy(f\"{ori_data_path}/{file.split('_')[0]}/{file}.TextGrid\", f\"{cache_path}{load_type}/text/{file}.TextGrid\")\n",
    "    except:\n",
    "        print(f\"{file}.TextGrid\")\n",
    "    try:\n",
    "        shutil.copy(f\"{ori_data_path_ann}/{file.split('_')[0]}/{file}.txt\", f\"{cache_path}{load_type}/sem/{file}.txt\")\n",
    "    except:\n",
    "        print(f\"{file}.txt\")\n",
    "    try:\n",
    "        shutil.copy(f\"{ori_data_path_ann}/{file.split('_')[0]}/{file}.csv\", f\"{cache_path}{load_type}/emo/{file}.csv\")\n",
    "    except:\n",
    "        print(f\"{file}.csv\")\n",
    "\n",
    "    npy_all.extend(list(np.load(f\"{ori_data_path_npy}/{file.split('_')[0]}/{file}.npy\")))\n",
    "\n",
    "    with open(f\"{ori_data_path}/{file.split('_')[0]}/{file}.json\", \"r\", encoding='utf-8') as json_file_raw:\n",
    "        json_file = json.load(json_file_raw)\n",
    "        with open(f\"{cache_path}{load_type}/facial52/{file}.json\", \"w\") as reduced_json:\n",
    "            counter = 0\n",
    "            new_frames_list = []\n",
    "            for json_data in json_file[\"frames\"]:\n",
    "                json_all.append(json_data[\"weights\"])\n",
    "                if counter % reduce_factor_json == 0:\n",
    "                    new_frames_list.append(json_data)\n",
    "                counter += 1\n",
    "            json_new = {\"names\": json_file[\"names\"], \"frames\": new_frames_list}\n",
    "            json.dump(json_new, reduced_json)\n",
    "\n",
    "        with open(f\"{ori_data_path}/{file.split('_')[0]}/{file}.bvh\", \"r\") as bvh_file:\n",
    "            with open(f\"{cache_path}{load_type}/bvh_rot/{file}.bvh\", \"w\") as reduced_raw_bvh:\n",
    "                with open(f\"{cache_path}{load_type}/bvh_full/{file}.bvh\", \"w\") as reduced_full_bvh:\n",
    "                    with open(f\"{cache_path}{load_type}/bvh_rot_vis/{file}.bvh\", \"w\") as reduced_trainable_bvh:\n",
    "                        for i, line_data in enumerate(bvh_file.readlines()):\n",
    "                            if i < 431: \n",
    "                                reduced_full_bvh.write(line_data)\n",
    "                                reduced_trainable_bvh.write(line_data)\n",
    "                            if i >= 431:\n",
    "                                data = np.fromstring(line_data, dtype=float, sep=' ')\n",
    "                                bvh_all.append(data)\n",
    "                                if i % reduce_factor_bvh == 0:\n",
    "                                    reduced_full_bvh.write(line_data)\n",
    "                                    trainable_rotation = np.zeros_like(data)\n",
    "                                    for k, v in target_list.items():\n",
    "                                        trainable_rotation[ori_list[k][1] - v:ori_list[k][1]] = data[ori_list[k][1] - v:ori_list[k][1]]\n",
    "\n",
    "                                    trainable_line_data = np.array2string(trainable_rotation, max_line_width=np.inf, precision=6, suppress_small=False, separator=' ')\n",
    "                                    reduced_trainable_bvh.write(trainable_line_data[1:-2] + \"\\n\")\n",
    "                                    data_rotation = np.zeros((1))   \n",
    "                                    for k, v in target_list.items():\n",
    "                                        data_rotation = np.concatenate((data_rotation, data[ori_list[k][1] - v:ori_list[k][1]]))                             \n",
    "                                        raw_line_data = np.array2string(data_rotation[1:], max_line_width=np.inf, precision=6, suppress_small=False, separator=' ')\n",
    "                                    reduced_raw_bvh.write(raw_line_data[1:-2] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #rot2pos\n",
    "# import time\n",
    "# start_t = time.time()\n",
    "# p = BVHParser()\n",
    "# data = [p.parse(\"../../../datasets/beat_full/1/1_wayne_0_1_8.bvh\")]\n",
    "# dr_pipe = Pipeline([\n",
    "#     ('param', MocapParameterizer('position')),\n",
    "# ])\n",
    "# xx = dr_pipe.fit_transform(data)\n",
    "# # data[0].values.shape\n",
    "# df = xx[0].values.head(-1)\n",
    "# print((time.time()-start_t)/60)\n",
    "# data_list = []\n",
    "# p_in_f = []\n",
    "# for joint, values in joint_list[\"beat_joints\"].items():\n",
    "#     x = df['%s_Xposition'%joint][-1]\n",
    "#     y = df['%s_Yposition'%joint][-1]\n",
    "#     z = df['%s_Zposition'%joint][-1]\n",
    "#     p = [x, y, z]\n",
    "#     p_in_f.append(p)\n",
    "# data_list.append(p_in_f)\n",
    "\n",
    "# %matplotlib inline\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# import matplotlib.pyplot as plt\n",
    "# draw = np.array(data_list[0])\n",
    "# fig = plt.figure()\n",
    "# ax = Axes3D(fig)\n",
    "# ax.scatter(draw.T[0], draw.T[1], draw.T[2], s=100, c='r')\n",
    "# plt.show()\n",
    "\n",
    "# json_str = {}\n",
    "# path = '../../../datasets/beat_full/'\n",
    "# for i in range(30):\n",
    "#     for bvh in os.listdir(path+str(i+1)+\"/\"):\n",
    "#         if bvh.endswith('.bvh'):\n",
    "#             p = BVHParser()\n",
    "#             filename = os.path.join(path, str(i+1), bvh)\n",
    "#             print(filename)\n",
    "#             data = [p.parse(filename)]\n",
    "\n",
    "#             dr_pipe = Pipeline([\n",
    "#                 ('param', MocapParameterizer('position')),\n",
    "#             ])\n",
    "\n",
    "#             xx = dr_pipe.fit_transform(data)\n",
    "#             data[0].values.shape\n",
    "#             df = xx[0].values.head(-1)\n",
    "\n",
    "#             data_list = []\n",
    "#             #counter = 0\n",
    "#             for f in range(data[0].values.shape[0]):\n",
    "#                 #if counter == 100: break\n",
    "#                 p_in_f = []\n",
    "#                 for joint, values in joint_list[\"beat_joints\"].items():\n",
    "#                     # print(joint)\n",
    "#                     x = df['%s_Xposition'%joint][f-1]\n",
    "#                     y = df['%s_Yposition'%joint][f-1]\n",
    "#                     z = df['%s_Zposition'%joint][f-1]\n",
    "#                     p = [x, y, z]\n",
    "#                     p_in_f.append(p)\n",
    "#                 #counter += 1\n",
    "#                 data_list.append(p_in_f)\n",
    "#             # print(np.array(data_list).shape)\n",
    "#             json_str[filename] = data_list\n",
    "#             print(json_str)\n",
    "#             # json_string = json.dumps(json_str)\n",
    "#             with open('position_data2.json', 'w') as outfile:\n",
    "#                 json.dump(json_str, outfile)\n",
    "#             break\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beat_preprocessing_kernel",
   "language": "python",
   "name": "beat_preprocessing_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
