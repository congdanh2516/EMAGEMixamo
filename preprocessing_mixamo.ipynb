{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from PyMO.pymo.parsers import BVHParser\n",
    "from PyMO.pymo.preprocessing import *\n",
    "from PyMO.pymo.viz_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e77ca62d6f0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pymo/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e77ca62d900>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pymo/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e77ca62db10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pymo/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pymo\n",
      "  Using cached pymo-0.2.0-py3-none-any.whl\n",
      "Requirement already satisfied: pymongo<4,>=2 in /root/.local/lib/python3.10/site-packages (from pymo) (3.13.0)\n",
      "Installing collected packages: pymo\n",
      "Successfully installed pymo-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pymo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/OMR-Research/pymo.git\n",
      "  Cloning https://github.com/OMR-Research/pymo.git to /tmp/pip-req-build-bfyq77qd\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/OMR-Research/pymo.git /tmp/pip-req-build-bfyq77qd\n",
      "Username for 'https://github.com': ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/OMR-Research/pymo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://****@github.com/iPERDance/pymo.git\n",
      "  Cloning https://****@github.com/iPERDance/pymo.git to /tmp/pip-req-build-0xaa93ng\n",
      "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/iPERDance/pymo.git' /tmp/pip-req-build-0xaa93ng\n",
      "  remote: Repository not found.\n",
      "  fatal: repository 'https://github.com/iPERDance/pymo.git/' not found\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[32m'https://****@github.com/iPERDance/pymo.git'\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/\u001b[0m\u001b[32mpip-req-build-0xaa93ng\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[32m'https://****@github.com/iPERDance/pymo.git'\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/\u001b[0m\u001b[32mpip-req-build-0xaa93ng\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://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@github.com/iPERDance/pymo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (1.21.5)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "\u001b[33m  WARNING: The scripts f2py and numpy-config are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_list = {\n",
    "    \"beat_joints\" : {\n",
    "        'Hips':         [6,6],\n",
    "        'Spine':        [3,9],\n",
    "        'Spine1':       [3,12],\n",
    "        'Spine2':       [3,15],\n",
    "        'Spine3':       [3,18],\n",
    "        'Neck':         [3,21],\n",
    "        'Neck1':        [3,24],\n",
    "        'Head':         [3,27],\n",
    "        'HeadEnd':      [3,30],\n",
    "\n",
    "        'RShoulder':    [3,33], \n",
    "        'RArm':         [3,36],\n",
    "        'RArm1':        [3,39],\n",
    "        'RHand':        [3,42],    \n",
    "        'RHandM1':      [3,45],\n",
    "        'RHandM2':      [3,48],\n",
    "        'RHandM3':      [3,51],\n",
    "        'RHandM4':      [3,54],\n",
    "\n",
    "        'RHandR':       [3,57],\n",
    "        'RHandR1':      [3,60],\n",
    "        'RHandR2':      [3,63],\n",
    "        'RHandR3':      [3,66],\n",
    "        'RHandR4':      [3,69],\n",
    "\n",
    "        'RHandP':       [3,72],\n",
    "        'RHandP1':      [3,75],\n",
    "        'RHandP2':      [3,78],\n",
    "        'RHandP3':      [3,81],\n",
    "        'RHandP4':      [3,84],\n",
    "\n",
    "        'RHandI':       [3,87],\n",
    "        'RHandI1':      [3,90],\n",
    "        'RHandI2':      [3,93],\n",
    "        'RHandI3':      [3,96],\n",
    "        'RHandI4':      [3,99],\n",
    "\n",
    "        'RHandT1':      [3,102],\n",
    "        'RHandT2':      [3,105],\n",
    "        'RHandT3':      [3,108],\n",
    "        'RHandT4':      [3,111],\n",
    "\n",
    "        'LShoulder':    [3,114], \n",
    "        'LArm':         [3,117],\n",
    "        'LArm1':        [3,120],\n",
    "        'LHand':        [3,123],    \n",
    "        'LHandM1':      [3,126],\n",
    "        'LHandM2':      [3,129],\n",
    "        'LHandM3':      [3,132],\n",
    "        'LHandM4':      [3,135],\n",
    "\n",
    "        'LHandR':       [3,138],\n",
    "        'LHandR1':      [3,141],\n",
    "        'LHandR2':      [3,144],\n",
    "        'LHandR3':      [3,147],\n",
    "        'LHandR4':      [3,150],\n",
    "\n",
    "        'LHandP':       [3,153],\n",
    "        'LHandP1':      [3,156],\n",
    "        'LHandP2':      [3,159],\n",
    "        'LHandP3':      [3,162],\n",
    "        'LHandP4':      [3,165],\n",
    "\n",
    "        'LHandI':       [3,168],\n",
    "        'LHandI1':      [3,171],\n",
    "        'LHandI2':      [3,174],\n",
    "        'LHandI3':      [3,177],\n",
    "        'LHandI4':      [3,180],\n",
    "\n",
    "        'LHandT1':      [3,183],\n",
    "        'LHandT2':      [3,186],\n",
    "        'LHandT3':      [3,189],\n",
    "        'LHandT4':      [3,192],\n",
    "\n",
    "        'RUpLeg':       [3,195],\n",
    "        'RLeg':         [3,198],\n",
    "        'RFoot':        [3,201],\n",
    "        'RFootF':       [3,204],\n",
    "        'RToeBase':     [3,207],\n",
    "        'RToeBaseEnd':  [3,210],\n",
    "\n",
    "        'LUpLeg':       [3,213],\n",
    "        'LLeg':         [3,216],\n",
    "        'LFoot':        [3,219],\n",
    "        'LFootF':       [3,222],\n",
    "        'LToeBase':     [3,225],\n",
    "        'LToeBaseEnd':  [3,228],\n",
    "        },\n",
    "\n",
    "    \"beat_full\" : {\n",
    "        'Hips':         6,\n",
    "        'Spine':        3,\n",
    "        'Spine1':       3,\n",
    "        'Spine2':       3,\n",
    "        'Spine3':       3,\n",
    "        'Neck':         3,\n",
    "        'Neck1':        3,\n",
    "        'Head':         3,\n",
    "        'HeadEnd':      3,\n",
    "\n",
    "        'RShoulder':    3,\n",
    "        'RArm':         3,\n",
    "        'RArm1':        3,\n",
    "        'RHand':        3,    \n",
    "        'RHandM1':      3,\n",
    "        'RHandM2':      3,\n",
    "        'RHandM3':      3,\n",
    "        'RHandM4':      3,\n",
    "\n",
    "        'RHandR':       3,\n",
    "        'RHandR1':      3,\n",
    "        'RHandR2':      3,\n",
    "        'RHandR3':      3,\n",
    "        'RHandR4':      3,\n",
    "\n",
    "        'RHandP':       3,\n",
    "        'RHandP1':      3,\n",
    "        'RHandP2':      3,\n",
    "        'RHandP3':      3,\n",
    "        'RHandP4':      3,\n",
    "\n",
    "        'RHandI':       3,\n",
    "        'RHandI1':      3,\n",
    "        'RHandI2':      3,\n",
    "        'RHandI3':      3,\n",
    "        'RHandI4':      3,\n",
    "\n",
    "        'RHandT1':      3,\n",
    "        'RHandT2':      3,\n",
    "        'RHandT3':      3,\n",
    "        'RHandT4':      3,\n",
    "\n",
    "        'LShoulder':    3, \n",
    "        'LArm':         3,\n",
    "        'LArm1':        3,\n",
    "        'LHand':        3,    \n",
    "        'LHandM1':      3,\n",
    "        'LHandM2':      3,\n",
    "        'LHandM3':      3,\n",
    "        'LHandM4':      3,\n",
    "\n",
    "        'LHandR':       3,\n",
    "        'LHandR1':      3,\n",
    "        'LHandR2':      3,\n",
    "        'LHandR3':      3,\n",
    "        'LHandR4':      3,\n",
    "\n",
    "        'LHandP':       3,\n",
    "        'LHandP1':      3,\n",
    "        'LHandP2':      3,\n",
    "        'LHandP3':      3,\n",
    "        'LHandP4':      3,\n",
    "\n",
    "        'LHandI':       3,\n",
    "        'LHandI1':      3,\n",
    "        'LHandI2':      3,\n",
    "        'LHandI3':      3,\n",
    "        'LHandI4':      3,\n",
    "\n",
    "        'LHandT1':      3,\n",
    "        'LHandT2':      3,\n",
    "        'LHandT3':      3,\n",
    "        'LHandT4':      3,\n",
    "\n",
    "        'RUpLeg':       3,\n",
    "        'RLeg':         3,\n",
    "        'RFoot':        3,\n",
    "        'RFootF':       3,\n",
    "        'RToeBase':     3,\n",
    "        'RToeBaseEnd':  3,\n",
    "\n",
    "        'LUpLeg':       3,\n",
    "        'LLeg':         3,\n",
    "        'LFoot':        3,\n",
    "        'LFootF':       3,\n",
    "        'LToeBase':     3,\n",
    "        'LToeBaseEnd':  3,\n",
    "    },\n",
    "    \n",
    "    \"beat_141\" : {\n",
    "            'Spine':       3 ,\n",
    "            'Neck':        3 ,\n",
    "            'Neck1':       3 ,\n",
    "            'RShoulder':   3 , \n",
    "            'RArm':        3 ,\n",
    "            'RArm1':       3 ,\n",
    "            'RHand':       3 ,    \n",
    "            'RHandM1':     3 ,\n",
    "            'RHandM2':     3 ,\n",
    "            'RHandM3':     3 ,\n",
    "            'RHandR':      3 ,\n",
    "            'RHandR1':     3 ,\n",
    "            'RHandR2':     3 ,\n",
    "            'RHandR3':     3 ,\n",
    "            'RHandP':      3 ,\n",
    "            'RHandP1':     3 ,\n",
    "            'RHandP2':     3 ,\n",
    "            'RHandP3':     3 ,\n",
    "            'RHandI':      3 ,\n",
    "            'RHandI1':     3 ,\n",
    "            'RHandI2':     3 ,\n",
    "            'RHandI3':     3 ,\n",
    "            'RHandT1':     3 ,\n",
    "            'RHandT2':     3 ,\n",
    "            'RHandT3':     3 ,\n",
    "            'LShoulder':   3 , \n",
    "            'LArm':        3 ,\n",
    "            'LArm1':       3 ,\n",
    "            'LHand':       3 ,    \n",
    "            'LHandM1':     3 ,\n",
    "            'LHandM2':     3 ,\n",
    "            'LHandM3':     3 ,\n",
    "            'LHandR':      3 ,\n",
    "            'LHandR1':     3 ,\n",
    "            'LHandR2':     3 ,\n",
    "            'LHandR3':     3 ,\n",
    "            'LHandP':      3 ,\n",
    "            'LHandP1':     3 ,\n",
    "            'LHandP2':     3 ,\n",
    "            'LHandP3':     3 ,\n",
    "            'LHandI':      3 ,\n",
    "            'LHandI1':     3 ,\n",
    "            'LHandI2':     3 ,\n",
    "            'LHandI3':     3 ,\n",
    "            'LHandT1':     3 ,\n",
    "            'LHandT2':     3 ,\n",
    "            'LHandT3':     3 ,\n",
    "        },\n",
    "    \n",
    "    \"beat_27\" : {\n",
    "            'Spine':       3 ,\n",
    "            'Neck':        3 ,\n",
    "            'Neck1':       3 ,\n",
    "            'RShoulder':   3 , \n",
    "            'RArm':        3 ,\n",
    "            'RArm1':       3 ,\n",
    "            'LShoulder':   3 , \n",
    "            'LArm':        3 ,\n",
    "            'LArm1':       3 ,     \n",
    "        },\n",
    "    \n",
    "    \"mixamo_joints\" : {\n",
    "        'Hips': [6, 6],\n",
    "        'Spine': [3, 9],\n",
    "        'Spine1': [3, 12],\n",
    "        'Spine2': [3, 15],\n",
    "        'LeftShoulder': [3, 18],\n",
    "        'LeftArm': [3, 21],\n",
    "        'LeftForeArm': [3, 24],\n",
    "        'LeftHand': [3, 27],\n",
    "        'LeftHandIndex1': [3, 30],\n",
    "        'LeftHandIndex2': [3, 33],\n",
    "        'LeftHandIndex3': [3, 36],\n",
    "\n",
    "        'LeftHandMiddle1': [3, 39],\n",
    "        'LeftHandMiddle2': [3, 42],\n",
    "        'LeftHandMiddle3': [3, 45],\n",
    "\n",
    "        'LeftHandPinky1': [3, 48],\n",
    "        'LeftHandPinky2': [3, 51],\n",
    "        'LeftHandPinky3': [3, 54],\n",
    "\n",
    "        'LeftHandRing1': [3, 57],\n",
    "        'LeftHandRing2': [3, 60],\n",
    "        'LeftHandRing3': [3, 63],\n",
    "\n",
    "        'LeftHandThumb1': [3, 66],\n",
    "        'LeftHandThumb2': [3, 69],\n",
    "        'LeftHandThumb3': [3, 72],\n",
    "\n",
    "        'Neck': [3, 75],\n",
    "        'Head': [3, 78],\n",
    "\n",
    "        'RightShoulder': [3, 81],\n",
    "        'RightArm': [3, 84],\n",
    "        'RightForeArm': [3, 87],\n",
    "        'RightHand': [3, 90],\n",
    "        'RightHandIndex1': [3, 93],\n",
    "        'RightHandIndex2': [3, 96],\n",
    "        'RightHandIndex3': [3, 99],\n",
    "\n",
    "        'RightHandMiddle1': [3, 102],\n",
    "        'RightHandMiddle2': [3, 105],\n",
    "        'RightHandMiddle3': [3, 108],\n",
    "\n",
    "        'RightHandPinky1': [3, 111],\n",
    "        'RightHandPinky2': [3, 114],\n",
    "        'RightHandPinky3': [3, 117],\n",
    "\n",
    "        'RightHandRing1': [3, 120],\n",
    "        'RightHandRing2': [3, 123],\n",
    "        'RightHandRing3': [3, 126],\n",
    "\n",
    "        'RightHandThumb1': [3, 129],\n",
    "        'RightHandThumb2': [3, 132],\n",
    "        'RightHandThumb3': [3, 135],\n",
    "\n",
    "        'LeftUpLeg': [3, 138],\n",
    "        'LeftLeg': [3, 141],\n",
    "        'LeftFoot': [3, 144],\n",
    "        'LeftToeBase': [3, 147],\n",
    "\n",
    "        'RightUpLeg': [3, 150],\n",
    "        'RightLeg': [3, 153],\n",
    "        'RightFoot': [3, 156],\n",
    "        'RightToeBase': [3, 159]\n",
    "    },\n",
    "\n",
    "    \"mixamo_full\" : {\n",
    "        'Hips': 6,\n",
    "        'Spine': 3,\n",
    "        'Spine1': 3,\n",
    "        'Spine2': 3,\n",
    "        'LeftShoulder': 3,\n",
    "        'LeftArm': 3,\n",
    "        'LeftForeArm': 3,\n",
    "        'Neck': 3,\n",
    "        'RightShoulder': 3,\n",
    "        'RightArm': 3,\n",
    "        'RightForeArm': 3,\n",
    "\n",
    "        'LeftHand': 3,\n",
    "        'LeftHandIndex1': 3,\n",
    "        'LeftHandIndex2': 3,\n",
    "        'LeftHandIndex3': 3,\n",
    "        'LeftHandMiddle1': 3,\n",
    "        'LeftHandMiddle2': 3,\n",
    "        'LeftHandMiddle3': 3,\n",
    "        'LeftHandPinky1': 3,\n",
    "        'LeftHandPinky2': 3,\n",
    "        'LeftHandPinky3': 3,\n",
    "        'LeftHandRing1': 3,\n",
    "        'LeftHandRing2': 3,\n",
    "        'LeftHandRing3': 3,\n",
    "        'LeftHandThumb1': 3,\n",
    "        'LeftHandThumb2': 3,\n",
    "        'LeftHandThumb3': 3,\n",
    "        'RightHand': 3,\n",
    "        'RightHandIndex1': 3,\n",
    "        'RightHandIndex2': 3,\n",
    "        'RightHandIndex3': 3,\n",
    "        'RightHandMiddle1': 3,\n",
    "        'RightHandMiddle2': 3,\n",
    "        'RightHandMiddle3': 3,\n",
    "        'RightHandPinky1': 3,\n",
    "        'RightHandPinky2': 3,\n",
    "        'RightHandPinky3': 3,\n",
    "        'RightHandRing1': 3,\n",
    "        'RightHandRing2': 3,\n",
    "        'RightHandRing3': 3,\n",
    "        'RightHandThumb1': 3,\n",
    "        'RightHandThumb2': 3,\n",
    "        'RightHandThumb3': 3,\n",
    "\n",
    "        'Head': 3,\n",
    "\n",
    "        'LeftUpLeg': 3,\n",
    "        'LeftLeg': 3,\n",
    "        'LeftFoot': 3,\n",
    "        'LeftToeBase': 3,\n",
    "        'RightUpLeg': 3,\n",
    "        'RightLeg': 3,\n",
    "        'RightFoot': 3,\n",
    "        'RightToeBase': 3\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_fps = 30\n",
    "ori_list = joint_list[\"mixamo_joints\"]\n",
    "target_list = joint_list[\"mixamo_full\"]\n",
    "ori_data_path = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_v0.2.1_partial/\"\n",
    "ori_data_path_npy = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_npy_partial/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data processing ...: 100% 1/1 [00:00<00:00, 422.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for speaker in tqdm(range(1,2), \"Data processing ...\"):\n",
    "    for f in os.listdir(ori_data_path+str(speaker)):\n",
    "        if(f.endswith('wav')):\n",
    "            audio_folder = os.path.join(ori_data_path_npy+str(speaker))\n",
    "            os.makedirs(audio_folder, exist_ok=True)\n",
    "            audioOut = os.path.join(ori_data_path_npy+str(speaker), f.replace('wav', 'npy'))\n",
    "            if(not os.path.exists(audioOut)):\n",
    "                a, sr = librosa.load(os.path.join(ori_data_path+str(speaker), f), sr=16000)\n",
    "                np.save(audioOut, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_fps: 30, reduce json 2, reduce bvh 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:01,  6.59it/s]\n"
     ]
    }
   ],
   "source": [
    "#calculate mean and build cache for data. \n",
    "target_fps = 30\n",
    "ori_list = joint_list[\"mixamo_joints\"]\n",
    "target_list = joint_list[\"mixamo_joints\"]\n",
    "ori_data_path = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_v0.2.1_partial/\"\n",
    "#wave cache from a = librosa.load(sr=16000) and np.save(a)\n",
    "ori_data_path_npy = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_npy_partial/\"\n",
    "ori_data_path_ann = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_v0.2.1_partial/\"\n",
    "cache_path = f\"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_{target_fps}_mixamo/\"\n",
    "reduce_factor_json = int(60/target_fps)\n",
    "reduce_factor_bvh = int(120/target_fps)\n",
    "print(f\"target_fps: {target_fps}, reduce json {reduce_factor_json}, reduce bvh {reduce_factor_bvh}\")\n",
    "speakers = sorted(os.listdir(ori_data_path),key=str,)\n",
    "\n",
    "npy_s_v = []\n",
    "npy_s_k = []\n",
    "json_s_v = []\n",
    "bvh_s_v = []\n",
    "\n",
    "load_type = \"train\"\n",
    "if not os.path.exists(f\"{cache_path}\"): \n",
    "    os.mkdir(cache_path)\n",
    "if not os.path.exists(f\"{cache_path}{load_type}/\"): \n",
    "    os.mkdir(f\"{cache_path}{load_type}/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/wave16k/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/bvh_rot/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/bvh_full/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/bvh_rot_vis/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/facial52/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/text/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/emo/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/sem/\")     \n",
    "\n",
    "for speaker in [1]: #range(1, 31):#replace to 1, 31 for all speakers\n",
    "    if speaker in []:\n",
    "        print(\"Skip: \", speaker)\n",
    "        # break\n",
    "    else:\n",
    "        all_data = os.listdir(ori_data_path_npy+str(speaker))\n",
    "        npy_all = []\n",
    "        json_all = []\n",
    "        bvh_all = []   \n",
    "        for ii, file in tqdm(enumerate(all_data)):\n",
    "            file = file[:-4]\n",
    "            npy_all.extend(list(np.load(f\"{ori_data_path_npy}/{file.split('_')[0]}/{file}.npy\")))\n",
    "\n",
    "        npy_all = np.array(npy_all)\n",
    "        npy_mean = np.mean(npy_all, axis=0)\n",
    "        npy_std = np.std(npy_all, axis=0)\n",
    "        npy_s_v.append([npy_mean, npy_std])\n",
    "        npy_s_k.append([len(npy_all)/16000/60])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [02:37, 17.53s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Function to process a single speaker\n",
    "def process_speaker(speaker):\n",
    "    if speaker in []:\n",
    "        print(\"Skip: \", speaker)\n",
    "        return\n",
    "    \n",
    "    ori_data_path_npy = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_npy_partial/\"\n",
    "    ori_data_path = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_v0.2.1_partial/\"\n",
    "    ori_data_path_ann = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_v0.2.1_partial/\"\n",
    "    cache_path = f\"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_{target_fps}_mixamo/\"\n",
    "    load_type = \"train\"\n",
    "\n",
    "    all_data = os.listdir(f\"{ori_data_path_npy}{speaker}\")\n",
    "    npy_all = []\n",
    "    json_all = []\n",
    "    bvh_all = []   \n",
    "\n",
    "    for ii, file in tqdm(enumerate(all_data)):\n",
    "        file = file[:-4]\n",
    "        shutil.copy(f\"{ori_data_path_npy}/{file.split('_')[0]}/{file}.npy\", f\"{cache_path}{load_type}/wave16k/{file}.npy\")\n",
    "        try:\n",
    "            shutil.copy(f\"{ori_data_path}/{file.split('_')[0]}/{file}.TextGrid\", f\"{cache_path}{load_type}/text/{file}.TextGrid\")\n",
    "        except:\n",
    "            print(f\"{file}.TextGrid\")\n",
    "        try:\n",
    "            shutil.copy(f\"{ori_data_path_ann}/{file.split('_')[0]}/{file}.txt\", f\"{cache_path}{load_type}/sem/{file}.txt\")\n",
    "        except:\n",
    "            print(f\"{file}.txt\")\n",
    "        try:\n",
    "            shutil.copy(f\"{ori_data_path_ann}/{file.split('_')[0]}/{file}.csv\", f\"{cache_path}{load_type}/emo/{file}.csv\")\n",
    "        except:\n",
    "            print(f\"{file}.csv\")\n",
    "\n",
    "        npy_all.extend(list(np.load(f\"{ori_data_path_npy}/{file.split('_')[0]}/{file}.npy\")))\n",
    "\n",
    "        with open(f\"{ori_data_path}/{file.split('_')[0]}/{file}.json\", \"r\", encoding='utf-8') as json_file_raw:\n",
    "            json_file = json.load(json_file_raw)\n",
    "            with open(f\"{cache_path}{load_type}/facial52/{file}.json\", \"w\") as reduced_json:\n",
    "                counter = 0\n",
    "                new_frames_list = []\n",
    "                for json_data in json_file[\"frames\"]:\n",
    "                    json_all.append(json_data[\"weights\"])\n",
    "                    if counter % reduce_factor_json == 0:\n",
    "                        new_frames_list.append(json_data)\n",
    "                    counter += 1\n",
    "                json_new = {\"names\": json_file[\"names\"], \"frames\": new_frames_list}\n",
    "                json.dump(json_new, reduced_json)\n",
    "\n",
    "            with open(f\"{ori_data_path}/{file.split('_')[0]}/{file}.bvh\", \"r\") as bvh_file:\n",
    "                with open(f\"{cache_path}{load_type}/bvh_rot/{file}.bvh\", \"w\") as reduced_raw_bvh:\n",
    "                    with open(f\"{cache_path}{load_type}/bvh_full/{file}.bvh\", \"w\") as reduced_full_bvh:\n",
    "                        with open(f\"{cache_path}{load_type}/bvh_rot_vis/{file}.bvh\", \"w\") as reduced_trainable_bvh:\n",
    "                            for i, line_data in enumerate(bvh_file.readlines()):\n",
    "                                if i < 431: \n",
    "                                    reduced_full_bvh.write(line_data)\n",
    "                                    reduced_trainable_bvh.write(line_data)\n",
    "                                if i >= 431:\n",
    "                                    data = np.fromstring(line_data, dtype=float, sep=' ')\n",
    "                                    bvh_all.append(data)\n",
    "                                    if i % reduce_factor_bvh == 0:\n",
    "                                        reduced_full_bvh.write(line_data)\n",
    "                                        trainable_rotation = np.zeros_like(data)\n",
    "                                        for k, v in target_list.items():\n",
    "                                            trainable_rotation[ori_list[k][1] - v:ori_list[k][1]] = data[ori_list[k][1] - v:ori_list[k][1]]\n",
    "\n",
    "                                        trainable_line_data = np.array2string(trainable_rotation, max_line_width=np.inf, precision=6, suppress_small=False, separator=' ')\n",
    "                                        reduced_trainable_bvh.write(trainable_line_data[1:-2] + \"\\n\")\n",
    "                                        data_rotation = np.zeros((1))   \n",
    "                                        for k, v in target_list.items():\n",
    "                                            data_rotation = np.concatenate((data_rotation, data[ori_list[k][1] - v:ori_list[k][1]]))                             \n",
    "                                            raw_line_data = np.array2string(data_rotation[1:], max_line_width=np.inf, precision=6, suppress_small=False, separator=' ')\n",
    "                                        reduced_raw_bvh.write(raw_line_data[1:-2] + \"\\n\")\n",
    "\n",
    "    npy_all = np.array(npy_all)\n",
    "    npy_mean = np.mean(npy_all, axis=0)\n",
    "    npy_std = np.std(npy_all, axis=0)\n",
    "    np.save(f\"{cache_path}{load_type}/wave16k/npy_mean_{speaker}.npy\", npy_mean)\n",
    "    np.save(f\"{cache_path}{load_type}/wave16k/npy_std_{speaker}.npy\", npy_std)  \n",
    "\n",
    "    json_all = np.array(json_all)\n",
    "    json_mean = np.mean(json_all, axis=0)\n",
    "    json_std = np.std(json_all, axis=0)\n",
    "    np.save(f\"{cache_path}{load_type}/facial52/json_mean_{speaker}.npy\", json_mean)\n",
    "    np.save(f\"{cache_path}{load_type}/facial52/json_std_{speaker}.npy\", json_std)\n",
    "\n",
    "    bvh_all = np.array(bvh_all)\n",
    "    bvh_mean = np.mean(bvh_all, axis=0)\n",
    "    bvh_std = np.std(bvh_all, axis=0)\n",
    "    data_rotation = np.zeros((1))\n",
    "    for k, v in target_list.items():\n",
    "        data_rotation = np.concatenate((data_rotation, bvh_mean[ori_list[k][1] - v:ori_list[k][1]]))\n",
    "    new_npy_mean = data_rotation[1:]\n",
    "    data_rotation = np.zeros((1))\n",
    "    for k, v in target_list.items():\n",
    "        data_rotation = np.concatenate((data_rotation, bvh_std[ori_list[k][1] - v:ori_list[k][1]]))\n",
    "    new_npy_std = data_rotation[1:]\n",
    "    np.save(f\"{cache_path}{load_type}/bvh_rot/bvh_mean_{speaker}.npy\", new_npy_mean)\n",
    "    np.save(f\"{cache_path}{load_type}/bvh_rot/bvh_std_{speaker}.npy\", new_npy_std)\n",
    "\n",
    "# Set up multiprocessing pool\n",
    "if __name__ == \"__main__\":\n",
    "    target_fps = 30\n",
    "    ori_list = joint_list[\"mixamo_joints\"]\n",
    "    target_list = joint_list[\"mixamo_full\"]\n",
    "    reduce_factor_json = int(60 / target_fps)\n",
    "    reduce_factor_bvh = int(120 / target_fps)\n",
    "    cache_path = f\"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_{target_fps}_mixamo/\"\n",
    "    \n",
    "    # Create required directories if they don't exist\n",
    "    load_type = \"train\"\n",
    "    if not os.path.exists(f\"{cache_path}{load_type}/\"):\n",
    "        os.makedirs(f\"{cache_path}{load_type}/wave16k/\")\n",
    "        os.makedirs(f\"{cache_path}{load_type}/bvh_rot/\")\n",
    "        os.makedirs(f\"{cache_path}{load_type}/bvh_full/\")\n",
    "        os.makedirs(f\"{cache_path}{load_type}/bvh_rot_vis/\")\n",
    "        os.makedirs(f\"{cache_path}{load_type}/facial52/\")\n",
    "        os.makedirs(f\"{cache_path}{load_type}/text/\")\n",
    "        os.makedirs(f\"{cache_path}{load_type}/emo/\")\n",
    "        os.makedirs(f\"{cache_path}{load_type}/sem/\")\n",
    "\n",
    "    # List of speakers to process\n",
    "    speakers = list(range(1, 2))\n",
    "\n",
    "    # Create a pool of workers equal to the number of CPUs\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        pool.map(process_speaker, speakers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = f\"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_{target_fps}_mixamo/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8.933333333333334]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npy_s_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16732743\n",
      " audio mean: -1.6127105482155457e-05, std: 0.13686008751392365\n",
      "[0.02775904 0.02777679 0.44012371 0.50456246 0.50450333 0.0029573\n",
      " 0.05623601 0.05412026 0.00730551 0.00732019 0.00852479 0.00858633\n",
      " 0.19716244 0.03872548 0.01715556 0.14934364 0.21650072 0.21810042\n",
      " 0.05286951 0.05287912 0.12554895 0.12562526 0.03882036 0.00480115\n",
      " 0.15803362 0.00680211 0.0927393  0.02075272 0.02182058 0.03754009\n",
      " 0.04585135 0.17647256 0.00115032 0.16912099 0.16241158 0.00791299\n",
      " 0.00899597 0.02440656 0.00136784 0.11179932 0.07947266 0.00887737\n",
      " 0.03710049 0.54662055 0.50104484 0.05239881 0.0636449  0.03850857\n",
      " 0.03656519 0.03719281 0.03820892]\n",
      " json mean: [0.01742299 0.01743268 0.43780211 0.35858555 0.35860627 0.02826202\n",
      " 0.16320558 0.17592981 0.00141975 0.00142111 0.00903972 0.00908408\n",
      " 0.15154494 0.03683376 0.00825738 0.08141314 0.19492    0.19545269\n",
      " 0.23159961 0.2317651  0.65696194 0.65691665 0.18594008 0.01654111\n",
      " 0.3015569  0.01483691 0.16577898 0.146761   0.14610192 0.02376963\n",
      " 0.0303267  0.24991706 0.00636894 0.3523207  0.36449022 0.05905783\n",
      " 0.06148917 0.07669842 0.00874356 0.11385674 0.09473449 0.05150478\n",
      " 0.24664048 0.26604066 0.23225579 0.33668237 0.36763061 0.08508344\n",
      " 0.08593307 0.12039033 0.12765256], std: [0.05574366 0.05576147 0.22196301 0.23765705 0.23764312 0.01819452\n",
      " 0.07934151 0.07783469 0.02859686 0.02862559 0.03089122 0.03100252\n",
      " 0.14856117 0.06584027 0.04382236 0.12929645 0.15567644 0.15625052\n",
      " 0.07693002 0.07693702 0.11854951 0.11858553 0.06592088 0.02318281\n",
      " 0.133005   0.02759403 0.1018885  0.04819819 0.04942269 0.06482476\n",
      " 0.07164226 0.14055032 0.01134755 0.13759162 0.13483472 0.02976211\n",
      " 0.03173344 0.05226928 0.01237404 0.11186979 0.09431963 0.03152357\n",
      " 0.06444408 0.24736384 0.23682717 0.0765868  0.08440634 0.06565562\n",
      " 0.06397749 0.06452421 0.06539967]\n",
      "\n",
      "\n",
      "[-5.34897083e+00  9.34698766e+01  1.06483513e+01  3.51425233e+00\n",
      " -2.36882613e+00  1.29504343e+00  1.21380205e+00  1.29084636e+00\n",
      " -6.81259573e-01  1.21456443e+00  1.29191944e+00 -6.81893678e-01\n",
      "  1.21741793e+00  1.29480431e+00 -6.83257154e-01  7.24981907e+01\n",
      "  8.72214345e+01 -2.63947188e+00  3.28730129e+01  6.86409682e+01\n",
      "  4.08717742e+01  2.61124000e+01  1.49129149e+00  8.62538715e+01\n",
      " -4.33196416e+00 -5.00948465e-01 -7.79581875e-01  1.10895527e+02\n",
      "  8.71311193e+01  3.97909446e+01 -3.80589427e+01  6.57510497e+01\n",
      " -5.16125643e+01 -1.80359255e+01  2.92592152e+01 -8.61541579e+01\n",
      "  7.09797656e+00 -2.77744771e+01  2.18225650e+01  2.99367137e+01\n",
      "  2.68581141e+01  8.79886154e+00  5.73520109e-01  2.25735933e+01\n",
      "  4.61187545e-05  4.01158581e-06  1.50917533e+01 -8.03113686e-04\n",
      "  6.27499599e+00  3.69011963e+01 -9.36795759e+00  4.71838796e+00\n",
      "  3.34537864e+01  3.02404354e+00  1.07393193e-04  2.25781887e+01\n",
      "  9.82983345e-05 -3.41096081e+01  2.03834485e+01 -4.87577078e+01\n",
      "  5.50021895e+00  4.11040340e+01  6.78200471e-01 -1.15018103e-04\n",
      "  2.75328352e+01 -6.16125996e-04 -2.63030147e+01  2.71046026e+01\n",
      " -3.73283228e+01  2.64592942e+00  4.12623901e+01  2.56772881e+00\n",
      "  9.72483707e-06  2.78224513e+01 -1.51672701e-04  5.43454972e+01\n",
      "  2.57043420e+01  4.93191157e+01 -2.33070565e+00 -4.38676638e+00\n",
      " -2.57531224e+01 -6.62013917e-01  7.44752010e+00 -7.57857211e+00\n",
      " -4.37855454e+01 -2.31672399e+01 -1.70816381e+01 -5.31685438e+00\n",
      "  2.48878061e+01  9.37748101e+00 -5.86599182e-01  2.73366797e+01\n",
      " -3.61052878e+00 -2.11151340e-05  1.85883665e+01  1.84214337e-04\n",
      "  1.80318318e+01  2.95350143e+01  2.94302428e+01 -1.17436990e+01\n",
      "  3.83311897e+01 -8.09422904e+00 -1.81462708e-05  2.62186050e+01\n",
      " -6.60970311e-05  4.91106648e+01  1.57096092e+01  5.38863002e+01\n",
      "  2.60652426e-01  3.03266938e+01 -2.60084721e-02 -4.23461260e-05\n",
      "  2.02895611e+01  2.16205648e-04  3.41460211e+01  1.99528355e+01\n",
      "  4.19077067e+01 -5.96960141e+00  3.44468809e+01 -1.82563266e-01\n",
      "  4.46777697e-06  2.30852302e+01 -4.36857350e-05 -2.97260501e+01\n",
      "  3.19022783e+01 -3.24335661e+01  2.85206807e+00 -2.51622274e+00\n",
      "  2.70526922e+01  7.37261093e-01  8.49295768e+00  8.63212513e+00\n",
      " -4.33196439e+00 -5.00948255e-01 -7.79582049e-01  3.24434917e+00\n",
      "  5.12770310e+00 -5.25187890e+01  1.70485445e-02 -3.83863129e+00\n",
      "  8.13928892e-02 -1.82565304e+01  7.06457208e+01 -1.09942110e+01\n",
      "  0.00000000e+00  2.53508180e+01  0.00000000e+00 -5.73865807e-01\n",
      " -5.28336077e+00  6.05421704e+01  2.06179464e-01 -1.94374981e+01\n",
      " -1.19356647e-01  6.49786788e+01  6.64064348e+01  2.28378255e+01\n",
      "  0.00000000e+00  2.71940922e+01 -9.97610427e-05] (159,)\n",
      "[8.65120366e+00 8.08790295e-01 8.57691578e+00 8.76355254e+00\n",
      " 2.43659429e+00 1.74982231e+00 1.04469950e+00 9.52749317e-01\n",
      " 9.60086369e-01 1.04578318e+00 9.54363546e-01 9.61562228e-01\n",
      " 1.04783726e+00 9.55624690e-01 9.62963052e-01 6.74878333e+01\n",
      " 3.71534738e+00 4.27989989e+01 3.56336382e+01 1.22733117e+01\n",
      " 3.27378953e+01 1.73406989e+01 8.57571908e+00 3.02276455e+01\n",
      " 5.39857804e+00 2.70986696e+00 1.64453332e+00 1.08234356e+02\n",
      " 2.59610363e+00 1.07732642e+02 3.76915453e+01 1.04291535e+01\n",
      " 2.96922914e+01 2.37115330e+01 1.63843652e+01 3.29076648e+01\n",
      " 3.03528108e+01 2.01528935e+01 2.30217728e+01 1.77460869e+01\n",
      " 2.94858294e+01 1.80825458e+01 1.43572338e+01 1.95487715e+01\n",
      " 7.77854090e-04 1.39258863e-04 1.31217135e+01 4.98429548e-04\n",
      " 5.15717985e+01 3.13968943e+01 3.48471236e+01 3.35756737e+01\n",
      " 2.50079775e+01 2.32343933e+01 9.51370653e-05 1.72142658e+01\n",
      " 4.14093674e-04 2.96645425e+01 2.64189509e+01 1.97048278e+01\n",
      " 4.27536372e+01 1.64158226e+01 1.10243914e+01 8.24696839e-05\n",
      " 1.11572134e+01 1.13223693e-04 3.11990162e+01 3.00145388e+01\n",
      " 2.72815356e+01 3.52630009e+01 2.30766632e+01 2.40330505e+01\n",
      " 1.11316249e-04 1.59422677e+01 1.95033624e-04 9.10839882e+00\n",
      " 3.57156471e+00 4.92324962e+00 2.74484247e+00 5.71593158e+00\n",
      " 1.00429935e+01 6.37977682e-01 4.26592509e+00 4.42031385e+00\n",
      " 2.90168172e+01 1.98423403e+01 1.44716793e+01 7.74228890e+00\n",
      " 2.39540347e+01 8.96894923e+00 4.16908419e+01 2.38517058e+01\n",
      " 2.52355532e+01 5.46483902e-05 1.67866236e+01 6.13501988e-05\n",
      " 2.31363647e+01 2.93826583e+01 1.91916274e+01 5.27541101e+01\n",
      " 2.61796660e+01 3.75523953e+01 1.96012484e-04 1.86516863e+01\n",
      " 2.51734983e-04 2.81076182e+01 2.41245957e+01 2.01508302e+01\n",
      " 1.03852664e+01 1.97090934e+01 2.16599506e+00 1.89518232e-04\n",
      " 1.32671632e+01 3.06718538e-04 3.01879486e+01 2.82380337e+01\n",
      " 2.48579350e+01 4.70812175e+01 2.44187158e+01 9.18831070e+00\n",
      " 3.15406286e-04 1.64734670e+01 5.27021771e-04 9.84674809e+00\n",
      " 3.93081836e+00 4.25830898e+00 2.28413284e+00 4.92643686e+00\n",
      " 6.14803299e+00 5.42901700e-01 3.18983080e+00 3.32986941e+00\n",
      " 5.39857833e+00 2.70986703e+00 1.64453349e+00 9.17117916e+00\n",
      " 5.25740195e+00 1.67556974e+02 1.97756083e-01 7.66751216e+00\n",
      " 5.07518558e-02 2.03328524e+01 3.55487690e+00 1.39681825e+01\n",
      " 0.00000000e+00 1.54478567e-02 0.00000000e+00 7.59956559e+00\n",
      " 3.96811403e+00 1.64196775e+02 2.76871158e-01 6.59382748e+00\n",
      " 7.24918901e-02 2.51935980e+01 6.21742682e+00 9.91482501e+00\n",
      " 0.00000000e+00 1.00206729e+00 4.88248166e-06] (159,)\n"
     ]
    }
   ],
   "source": [
    "npy_s_v = []\n",
    "json_s_v = []\n",
    "bvh_s_v = []\n",
    "\n",
    "npy_path = f\"{cache_path}{load_type}/wave16k/\"\n",
    "bvh_path = f\"{cache_path}{load_type}/bvh_rot/\"\n",
    "json_path = f\"{cache_path}{load_type}/facial52/\" \n",
    "        \n",
    "for i in [1]: #range(1, 31):\n",
    "    npy_s_v.append([np.load(f\"{npy_path}npy_mean_{i}.npy\"), np.load(f\"{npy_path}npy_std_{i}.npy\")])\n",
    "    json_s_v.append([np.load(f\"{json_path}json_mean_{i}.npy\"), np.load(f\"{json_path}json_std_{i}.npy\")])\n",
    "    bvh_s_v.append([np.load(f\"{bvh_path}bvh_mean_{i}.npy\"), np.load(f\"{bvh_path}bvh_std_{i}.npy\")])\n",
    "\n",
    "all_length = 0\n",
    "new_m = np.zeros_like(npy_s_v[0][0])\n",
    "new_s = np.zeros_like(npy_s_v[0][0])\n",
    "for i, (m, s) in enumerate(npy_s_v):\n",
    "    all_length += npy_s_k[i][0]\n",
    "    new_m += npy_s_k[i][0] * m\n",
    "new_m /= all_length\n",
    "for i, (m, s) in enumerate(npy_s_v):\n",
    "    new_s += ((s**2) + (m-new_m)**2) * npy_s_k[i][0]\n",
    "\n",
    "print(new_s)\n",
    "new_s /= all_length\n",
    "new_s = np.sqrt(new_s)\n",
    "print(f\" audio mean: {new_m}, std: {new_s}\") \n",
    "np.save(f\"{npy_path}npy_mean.npy\", new_m)\n",
    "np.save(f\"{npy_path}/npy_std.npy\", new_s)  \n",
    "\n",
    "new_m = np.zeros_like(json_s_v[0][0])\n",
    "new_s = np.zeros_like(json_s_v[0][0])\n",
    "all_length = 0\n",
    "for i, (m, s) in enumerate(json_s_v):\n",
    "    all_length += npy_s_k[i][0]\n",
    "    new_m += npy_s_k[i][0] * m\n",
    "new_m /= all_length\n",
    "for i, (m, s) in enumerate(json_s_v):\n",
    "    new_s += ((s**2) + (m-new_m)**2) * npy_s_k[i][0]\n",
    "\n",
    "print(new_s)\n",
    "new_s /= all_length\n",
    "new_s = np.sqrt(new_s)\n",
    "print(f\" json mean: {new_m}, std: {new_s}\") \n",
    "np.save(f\"{json_path}json_mean.npy\", new_m)\n",
    "np.save(f\"{json_path}/json_std.npy\", new_s)\n",
    "\n",
    "new_m = np.zeros_like(bvh_s_v[0][0])\n",
    "new_s = np.zeros_like(bvh_s_v[0][0])\n",
    "all_length = 0\n",
    "for i, (m, s) in enumerate(bvh_s_v):\n",
    "    all_length += npy_s_k[i][0]\n",
    "    new_m += npy_s_k[i][0] * m\n",
    "new_m /= all_length\n",
    "for i, (m, s) in enumerate(bvh_s_v):\n",
    "    new_s += ((s**2) + (m-new_m)**2) * npy_s_k[i][0]\n",
    "new_s /= all_length\n",
    "new_s = np.sqrt(new_s)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(new_m, new_m.shape)\n",
    "print(new_s, new_s.shape)\n",
    "np.save(f\"{bvh_path}bvh_mean.npy\", new_m)\n",
    "np.save(f\"{bvh_path}/bvh_std.npy\", new_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_rule_english = {\n",
    "    # 4h speakers x 10\n",
    "    \"1, 2, 3, 4, 6, 7, 8, 9, 11\":{\n",
    "        # 48+40+100=188mins each\n",
    "        \"train\": [\n",
    "            \"0_9_9\", \"0_10_10\", \"0_11_11\", \"0_12_12\", \"0_13_13\", \"0_14_14\", \"0_15_15\", \"0_16_16\", \\\n",
    "            \"0_17_17\", \"0_18_18\", \"0_19_19\", \"0_20_20\", \"0_21_21\", \"0_22_22\", \"0_23_23\", \"0_24_24\", \\\n",
    "            \"0_25_25\", \"0_26_26\", \"0_27_27\", \"0_28_28\", \"0_29_29\", \"0_30_30\", \"0_31_31\", \"0_32_32\", \\\n",
    "            \"0_33_33\", \"0_34_34\", \"0_35_35\", \"0_36_36\", \"0_37_37\", \"0_38_38\", \"0_39_39\", \"0_40_40\", \\\n",
    "            \"0_41_41\", \"0_42_42\", \"0_43_43\", \"0_44_44\", \"0_45_45\", \"0_46_46\", \"0_47_47\", \"0_48_48\", \\\n",
    "            \"0_49_49\", \"0_50_50\", \"0_51_51\", \"0_52_52\", \"0_53_53\", \"0_54_54\", \"0_55_55\", \"0_56_56\", \\\n",
    "            \n",
    "            \"0_66_66\", \"0_67_67\", \"0_68_68\", \"0_69_69\", \"0_70_70\", \"0_71_71\",  \\\n",
    "            \"0_74_74\", \"0_75_75\", \"0_76_76\", \"0_77_77\", \"0_78_78\", \"0_79_79\",  \\\n",
    "            \"0_82_82\", \"0_83_83\", \"0_84_84\", \"0_85_85\",  \\\n",
    "            \"0_88_88\", \"0_89_89\", \"0_90_90\", \"0_91_91\", \"0_92_92\", \"0_93_93\",  \\\n",
    "            \"0_96_96\", \"0_97_97\", \"0_98_98\", \"0_99_99\", \"0_100_100\", \"0_101_101\",  \\\n",
    "            \"0_104_104\", \"0_105_105\", \"0_106_106\", \"0_107_107\", \"0_108_108\", \"0_109_109\",  \\\n",
    "            \"0_112_112\", \"0_113_113\", \"0_114_114\", \"0_115_115\", \"0_116_116\", \"0_117_117\",  \\\n",
    "            \n",
    "            \"1_2_2\", \"1_3_3\", \"1_4_4\", \"1_5_5\", \"1_6_6\", \"1_7_7\", \"1_8_8\", \"1_9_9\", \"1_10_10\", \"1_11_11\",\n",
    "        ],\n",
    "        # 8+7+10=25mins each\n",
    "        \"val\": [\n",
    "            \"0_57_57\", \"0_58_58\", \"0_59_59\", \"0_60_60\", \"0_61_61\", \"0_62_62\", \"0_63_63\", \"0_64_64\", \\\n",
    "            \"0_72_72\", \"0_80_80\", \"0_86_86\", \"0_94_94\", \"0_102_102\", \"0_110_110\", \"0_118_118\", \\\n",
    "            \"1_12_12\",\n",
    "        ],\n",
    "        # 8+7+10=25mins each\n",
    "        \"test\": [\n",
    "           \"0_1_1\", \"0_2_2\", \"0_3_3\", \"0_4_4\", \"0_5_5\", \"0_6_6\", \"0_7_7\", \"0_8_8\", \\\n",
    "           \"0_65_65\", \"0_73_73\", \"0_81_81\", \"0_87_87\", \"0_95_95\", \"0_103_103\", \"0_111_111\", \\\n",
    "           \"1_1_1\",\n",
    "        ],\n",
    "    },\n",
    "    \n",
    "    # 1h speakers x 20\n",
    "    \"5, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30\":{\n",
    "        # 8+7+20=35mins each\n",
    "        \"train\": [\n",
    "            \"0_9_9\", \"0_10_10\", \"0_11_11\", \"0_12_12\", \"0_13_13\", \"0_14_14\", \"0_15_15\", \"0_16_16\", \\\n",
    "            \"0_66_66\", \"0_74_74\", \"0_82_82\", \"0_88_88\", \"0_96_96\", \"0_104_104\", \"0_112_112\", \"0_118_118\", \\\n",
    "            \"1_2_2\", \"1_3_3\", \n",
    "            \"1_0_0\", \"1_4_4\", # for speaker 29 only\n",
    "        ],\n",
    "        # 4+3.5+5 = 12.5mins each\n",
    "        # 0_65_a and 0_65_b denote the frist and second half of sequence 0_65_65\n",
    "        \"val\": [\n",
    "            \"0_5_5\", \"0_6_6\", \"0_7_7\", \"0_8_8\",  \\\n",
    "            \"0_65_b\", \"0_73_b\", \"0_81_b\", \"0_87_b\", \"0_95_b\", \"0_103_b\", \"0_111_b\", \\\n",
    "            \"1_1_b\",\n",
    "        ],\n",
    "        # 4+3.5+5 = 12.5mins each\n",
    "        \"test\": [\n",
    "           \"0_1_1\", \"0_2_2\", \"0_3_3\", \"0_4_4\", \\\n",
    "           \"0_65_a\", \"0_73_a\", \"0_81_a\", \"0_87_a\", \"0_95_a\", \"0_103_a\", \"0_111_a\", \\\n",
    "           \"1_1_a\",\n",
    "        ],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_rule_english = {\n",
    "    # 4h speakers x 10\n",
    "    \"1, 2, 3, 4, 6, 7, 8, 9, 11\":{\n",
    "        # 48+40+100=188mins each\n",
    "        \"train\": [\n",
    "            \"0_1_1\", \"0_10_10\", \"0_100_100\", \"0_101_101\", \"0_102_102\",\n",
    "        ],\n",
    "        # 8+7+10=25mins each\n",
    "        \"val\": [\n",
    "            \"0_103_103\", \"0_104_104\", \n",
    "        ],\n",
    "        # 8+7+10=25mins each\n",
    "        \"test\": [\n",
    "           \"0_105_105\", \"0_106_106\",\n",
    "        ],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_sequence(source_path, save_path_a, save_path_b, file_id, fps = 15, sr = 16000, tmp=\"/data/nas07/PersonalData/danh/PantoMatrix/beat_tmp7/\"):\n",
    "    if not os.path.exists(tmp): os.mkdir(tmp)\n",
    "    cut_point = 30 if file_id.split(\"_\")[0] == \"0\" else 300 #in seconds\n",
    "    if source_path.endswith(\".npy\"):\n",
    "        data = np.load(source_path)\n",
    "        data_a = data[:sr*cut_point]\n",
    "        data_b = data[sr*cut_point:]\n",
    "        np.save(save_path_a, data_a)\n",
    "        np.save(save_path_b, data_b)\n",
    "        \n",
    "    elif source_path.endswith(\".bvh\"):\n",
    "        copy_lines = 431 if \"full\" in source_path or \"vis\" in source_path else 0\n",
    "        with open(source_path, \"r\") as data:\n",
    "            with open(save_path_a, \"w\") as data_a:\n",
    "                with open(save_path_b, \"w\") as data_b:\n",
    "                    for i, line_data in enumerate(data.readlines()):\n",
    "                        if i < copy_lines:\n",
    "                            data_a.write(line_data)\n",
    "                            data_b.write(line_data)\n",
    "                        elif i < cut_point * fps:\n",
    "                            data_a.write(line_data)\n",
    "                        else:\n",
    "                            data_b.write(line_data)\n",
    "    \n",
    "    elif source_path.endswith(\".json\"):\n",
    "        with open(source_path, \"r\", encoding='utf-8') as data:\n",
    "            json_file = json.load(data)\n",
    "            with open(save_path_a, \"w\") as data_a:\n",
    "                with open(save_path_b, \"w\") as data_b:\n",
    "                    new_frames_a = []\n",
    "                    new_frames_b = []\n",
    "                    for json_data in json_file[\"frames\"]:\n",
    "                        if json_data[\"time\"] < cut_point:\n",
    "                            new_frames_a.append(json_data)\n",
    "                        else:\n",
    "                            new_frame = json_data.copy()\n",
    "                            new_frame[\"time\"]-=cut_point\n",
    "                            new_frames_b.append(new_frame)\n",
    "                    json_new_a = {\"names\":json_file[\"names\"], \"frames\": new_frames_a}\n",
    "                    json_new_b = {\"names\":json_file[\"names\"], \"frames\": new_frames_b}\n",
    "                    json.dump(json_new_a, data_a)\n",
    "                    json.dump(json_new_b, data_b) \n",
    "        \n",
    "    else:\n",
    "        # processing in the dataloader\n",
    "        shutil.copy(source_path, save_path_a)\n",
    "        shutil.copy(source_path, save_path_b)\n",
    "    try:\n",
    "        shutil.move(source_path, tmp)\n",
    "    except:\n",
    "        print(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'default_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[43mdefault_path\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mfolder)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'default_path' is not defined"
     ]
    }
   ],
   "source": [
    "os.listdir(default_path+\"/\"+folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mroot\u001b[49m\u001b[38;5;241m/\u001b[39mnguyen\u001b[38;5;241m/\u001b[39mresearch\u001b[38;5;241m/\u001b[39mPantoMatrix\u001b[38;5;241m/\u001b[39mbeat_4english_30_full\u001b[38;5;241m/\u001b[39mtrain_copy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'root' is not defined"
     ]
    }
   ],
   "source": [
    "/root/nguyen/research/PantoMatrix/beat_4english_30_full/train_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_30_mixamo/train_copy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bvh_rot',\n",
       " 'facial52',\n",
       " 'wave16k',\n",
       " 'emo',\n",
       " 'bvh_full',\n",
       " 'sem',\n",
       " 'text',\n",
       " 'bvh_rot_vis']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = os.listdir(default_path)\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'txt'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(default_path+\"/\"+folders[5])[7].split(\".\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_30_mixamo/train_copy'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_30_mixamo/train_copy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bvh_rot',\n",
       " 'facial52',\n",
       " 'wave16k',\n",
       " 'emo',\n",
       " 'bvh_full',\n",
       " 'sem',\n",
       " 'text',\n",
       " 'bvh_rot_vis']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(default_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1/1 [00:01<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# spilt data\n",
    "speaker_names = [\n",
    "    \"wayne\", \"scott\", \"solomon\", \"lawrence\", \"stewart\", \"carla\", \"sophie\", \"catherine\", \"miranda\", \"kieks\", \\\n",
    "    \"nidal\", \"zhao\", \"lu\", \"zhang\", \"carlos\", \"jorge\", \"itoi\", \"daiki\", \"jaime\", \"li\", \\\n",
    "    \"ayana\", \"luqi\", \"hailing\", \"kexin\", \"goto\", \"reamey\", \"yingqing\", \"tiffnay\", \"hanieh\", \"katya\",\n",
    "]\n",
    "default_path = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_30_mixamo/train_copy\"\n",
    "four_hour_speakers = \"1, 2, 3, 4, 6, 7, 8, 9, 11\".split(\", \")\n",
    "one_hour_speakers = \"5, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30\".split(\", \")\n",
    "folders = os.listdir(default_path)\n",
    "if not os.path.exists(default_path.replace(\"train\", \"val\")): os.mkdir(default_path.replace(\"train\", \"val\"))\n",
    "if not os.path.exists(default_path.replace(\"train\", \"test\")): os.mkdir(default_path.replace(\"train\", \"test\"))\n",
    "endwith = []\n",
    "for folder in folders:\n",
    "    # print(os.listdir(default_path+\"/\"+folder))\n",
    "    # print(\"\\n\\n\")\n",
    "    # print(f\"folder: {(os.listdir(default_path + '/' + folder))[0]}\")\n",
    "    if not os.path.exists(default_path.replace(\"train\", \"val\")+\"/\"+folder): os.mkdir(default_path.replace(\"train\", \"val\")+\"/\"+folder)\n",
    "    if not os.path.exists(default_path.replace(\"train\", \"test\")+\"/\"+folder): os.mkdir(default_path.replace(\"train\", \"test\")+\"/\"+folder)\n",
    "    endwith.append(os.listdir(default_path+\"/\"+folder)[0].split(\".\")[-1])\n",
    "    \n",
    "for speaker_id in tqdm([1]):\n",
    "    val = split_rule_english[\"5, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30\"][\"val\"] if str(speaker_id) in one_hour_speakers else split_rule_english[\"1, 2, 3, 4, 6, 7, 8, 9, 11\"][\"val\"]\n",
    "    test = split_rule_english[\"5, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30\"][\"test\"] if str(speaker_id) in one_hour_speakers else split_rule_english[\"1, 2, 3, 4, 6, 7, 8, 9, 11\"][\"test\"]\n",
    "    for file_id in val:\n",
    "        for ide, folder in enumerate(folders):\n",
    "            if \"b\" in file_id:\n",
    "                print(f\"DATA: {default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id.split('_')[0]}_{file_id.split('_')[1]}_{file_id.split('_')[1]}.{endwith[ide]}\")\n",
    "                cut_sequence(\n",
    "                    source_path=f\"{default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id.split('_')[0]}_{file_id.split('_')[1]}_{file_id.split('_')[1]}.{endwith[ide]}\",\n",
    "                    save_path_a=f\"{default_path.replace('train', 'test')}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id.split('_')[0]}_{file_id.split('_')[1]}_a.{endwith[ide]}\",\n",
    "                    save_path_b=f\"{default_path.replace('train', 'val')}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id.split('_')[0]}_{file_id.split('_')[1]}_b.{endwith[ide]}\",\n",
    "                    file_id = file_id,\n",
    "                        )\n",
    "            else:\n",
    "                #pass\n",
    "                try:\n",
    "                    shutil.move(f\"{default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id}.{endwith[ide]}\", f\"{default_path.replace('train', 'val')}/{folder}/\")\n",
    "                except:\n",
    "                    print(f\"{default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id}.{endwith[ide]}\")\n",
    "    for file_id in test:\n",
    "        for ide, folder in enumerate(folders):\n",
    "            if \"a\" in file_id:\n",
    "                pass\n",
    "            else:\n",
    "                #pass\n",
    "                try:\n",
    "                    shutil.move(f\"{default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id}.{endwith[ide]}\", f\"{default_path.replace('train', 'test')}/{folder}/\")\n",
    "                except:\n",
    "                    print(f\"{default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id}.{endwith[ide]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1_wayne_0_106_106.npy'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_data_path_npy = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_npy_partial/\"\n",
    "all_data = os.listdir(f\"{ori_data_path_npy}{1}\")\n",
    "all_data[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_data_path_npy = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_npy_partial/\"\n",
    "ori_data_path = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_v0.2.1_partial/\"\n",
    "ori_data_path_ann = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_v0.2.1_partial/\"\n",
    "cache_path = f\"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_30_mixamo/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [01:48, 18.09s/it]"
     ]
    }
   ],
   "source": [
    "npy_all = []\n",
    "for ii, file in tqdm(enumerate(all_data)):\n",
    "    file = file[:-4]\n",
    "    shutil.copy(f\"{ori_data_path_npy}/{file.split('_')[0]}/{file}.npy\", f\"{cache_path}{load_type}/wave16k/{file}.npy\")\n",
    "    try:\n",
    "        shutil.copy(f\"{ori_data_path}/{file.split('_')[0]}/{file}.TextGrid\", f\"{cache_path}{load_type}/text/{file}.TextGrid\")\n",
    "    except:\n",
    "        print(f\"{file}.TextGrid\")\n",
    "    try:\n",
    "        shutil.copy(f\"{ori_data_path_ann}/{file.split('_')[0]}/{file}.txt\", f\"{cache_path}{load_type}/sem/{file}.txt\")\n",
    "    except:\n",
    "        print(f\"{file}.txt\")\n",
    "    try:\n",
    "        shutil.copy(f\"{ori_data_path_ann}/{file.split('_')[0]}/{file}.csv\", f\"{cache_path}{load_type}/emo/{file}.csv\")\n",
    "    except:\n",
    "        print(f\"{file}.csv\")\n",
    "\n",
    "    npy_all.extend(list(np.load(f\"{ori_data_path_npy}/{file.split('_')[0]}/{file}.npy\")))\n",
    "\n",
    "    with open(f\"{ori_data_path}/{file.split('_')[0]}/{file}.json\", \"r\", encoding='utf-8') as json_file_raw:\n",
    "        json_file = json.load(json_file_raw)\n",
    "        with open(f\"{cache_path}{load_type}/facial52/{file}.json\", \"w\") as reduced_json:\n",
    "            counter = 0\n",
    "            new_frames_list = []\n",
    "            for json_data in json_file[\"frames\"]:\n",
    "                json_all.append(json_data[\"weights\"])\n",
    "                if counter % reduce_factor_json == 0:\n",
    "                    new_frames_list.append(json_data)\n",
    "                counter += 1\n",
    "            json_new = {\"names\": json_file[\"names\"], \"frames\": new_frames_list}\n",
    "            json.dump(json_new, reduced_json)\n",
    "\n",
    "        with open(f\"{ori_data_path}/{file.split('_')[0]}/{file}.bvh\", \"r\") as bvh_file:\n",
    "            with open(f\"{cache_path}{load_type}/bvh_rot/{file}.bvh\", \"w\") as reduced_raw_bvh:\n",
    "                with open(f\"{cache_path}{load_type}/bvh_full/{file}.bvh\", \"w\") as reduced_full_bvh:\n",
    "                    with open(f\"{cache_path}{load_type}/bvh_rot_vis/{file}.bvh\", \"w\") as reduced_trainable_bvh:\n",
    "                        for i, line_data in enumerate(bvh_file.readlines()):\n",
    "                            if i < 431: \n",
    "                                reduced_full_bvh.write(line_data)\n",
    "                                reduced_trainable_bvh.write(line_data)\n",
    "                            if i >= 431:\n",
    "                                data = np.fromstring(line_data, dtype=float, sep=' ')\n",
    "                                bvh_all.append(data)\n",
    "                                if i % reduce_factor_bvh == 0:\n",
    "                                    reduced_full_bvh.write(line_data)\n",
    "                                    trainable_rotation = np.zeros_like(data)\n",
    "                                    for k, v in target_list.items():\n",
    "                                        trainable_rotation[ori_list[k][1] - v:ori_list[k][1]] = data[ori_list[k][1] - v:ori_list[k][1]]\n",
    "\n",
    "                                    trainable_line_data = np.array2string(trainable_rotation, max_line_width=np.inf, precision=6, suppress_small=False, separator=' ')\n",
    "                                    reduced_trainable_bvh.write(trainable_line_data[1:-2] + \"\\n\")\n",
    "                                    data_rotation = np.zeros((1))   \n",
    "                                    for k, v in target_list.items():\n",
    "                                        data_rotation = np.concatenate((data_rotation, data[ori_list[k][1] - v:ori_list[k][1]]))                             \n",
    "                                        raw_line_data = np.array2string(data_rotation[1:], max_line_width=np.inf, precision=6, suppress_small=False, separator=' ')\n",
    "                                    reduced_raw_bvh.write(raw_line_data[1:-2] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #rot2pos\n",
    "# import time\n",
    "# start_t = time.time()\n",
    "# p = BVHParser()\n",
    "# data = [p.parse(\"../../../datasets/beat_full/1/1_wayne_0_1_8.bvh\")]\n",
    "# dr_pipe = Pipeline([\n",
    "#     ('param', MocapParameterizer('position')),\n",
    "# ])\n",
    "# xx = dr_pipe.fit_transform(data)\n",
    "# # data[0].values.shape\n",
    "# df = xx[0].values.head(-1)\n",
    "# print((time.time()-start_t)/60)\n",
    "# data_list = []\n",
    "# p_in_f = []\n",
    "# for joint, values in joint_list[\"beat_joints\"].items():\n",
    "#     x = df['%s_Xposition'%joint][-1]\n",
    "#     y = df['%s_Yposition'%joint][-1]\n",
    "#     z = df['%s_Zposition'%joint][-1]\n",
    "#     p = [x, y, z]\n",
    "#     p_in_f.append(p)\n",
    "# data_list.append(p_in_f)\n",
    "\n",
    "# %matplotlib inline\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# import matplotlib.pyplot as plt\n",
    "# draw = np.array(data_list[0])\n",
    "# fig = plt.figure()\n",
    "# ax = Axes3D(fig)\n",
    "# ax.scatter(draw.T[0], draw.T[1], draw.T[2], s=100, c='r')\n",
    "# plt.show()\n",
    "\n",
    "# json_str = {}\n",
    "# path = '../../../datasets/beat_full/'\n",
    "# for i in range(30):\n",
    "#     for bvh in os.listdir(path+str(i+1)+\"/\"):\n",
    "#         if bvh.endswith('.bvh'):\n",
    "#             p = BVHParser()\n",
    "#             filename = os.path.join(path, str(i+1), bvh)\n",
    "#             print(filename)\n",
    "#             data = [p.parse(filename)]\n",
    "\n",
    "#             dr_pipe = Pipeline([\n",
    "#                 ('param', MocapParameterizer('position')),\n",
    "#             ])\n",
    "\n",
    "#             xx = dr_pipe.fit_transform(data)\n",
    "#             data[0].values.shape\n",
    "#             df = xx[0].values.head(-1)\n",
    "\n",
    "#             data_list = []\n",
    "#             #counter = 0\n",
    "#             for f in range(data[0].values.shape[0]):\n",
    "#                 #if counter == 100: break\n",
    "#                 p_in_f = []\n",
    "#                 for joint, values in joint_list[\"beat_joints\"].items():\n",
    "#                     # print(joint)\n",
    "#                     x = df['%s_Xposition'%joint][f-1]\n",
    "#                     y = df['%s_Yposition'%joint][f-1]\n",
    "#                     z = df['%s_Zposition'%joint][f-1]\n",
    "#                     p = [x, y, z]\n",
    "#                     p_in_f.append(p)\n",
    "#                 #counter += 1\n",
    "#                 data_list.append(p_in_f)\n",
    "#             # print(np.array(data_list).shape)\n",
    "#             json_str[filename] = data_list\n",
    "#             print(json_str)\n",
    "#             # json_string = json.dumps(json_str)\n",
    "#             with open('position_data2.json', 'w') as outfile:\n",
    "#                 json.dump(json_str, outfile)\n",
    "#             break\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beat_preprocessing",
   "language": "python",
   "name": "beat_preprocessing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
