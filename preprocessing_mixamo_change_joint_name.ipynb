{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from PyMO.pymo.parsers import BVHParser\n",
    "from PyMO.pymo.preprocessing import *\n",
    "from PyMO.pymo.viz_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n"
     ]
    }
   ],
   "source": [
    "!pip install pymo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/OMR-Research/pymo.git\n",
      "  Cloning https://github.com/OMR-Research/pymo.git to /tmp/pip-req-build-bfyq77qd\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/OMR-Research/pymo.git /tmp/pip-req-build-bfyq77qd\n",
      "Username for 'https://github.com': ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/OMR-Research/pymo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://****@github.com/iPERDance/pymo.git\n",
      "  Cloning https://****@github.com/iPERDance/pymo.git to /tmp/pip-req-build-0xaa93ng\n",
      "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/iPERDance/pymo.git' /tmp/pip-req-build-0xaa93ng\n",
      "  remote: Repository not found.\n",
      "  fatal: repository 'https://github.com/iPERDance/pymo.git/' not found\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[32m'https://****@github.com/iPERDance/pymo.git'\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/\u001b[0m\u001b[32mpip-req-build-0xaa93ng\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[32m'https://****@github.com/iPERDance/pymo.git'\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/\u001b[0m\u001b[32mpip-req-build-0xaa93ng\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://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@github.com/iPERDance/pymo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (1.21.5)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "\u001b[33m  WARNING: The scripts f2py and numpy-config are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_list = {\n",
    "    \"beat_joints\" : {\n",
    "        'Hips':         [6,6],\n",
    "        'Spine':        [3,9],\n",
    "        'Spine1':       [3,12],\n",
    "        'Spine2':       [3,15],\n",
    "        'Spine3':       [3,18],\n",
    "        'Neck':         [3,21],\n",
    "        'Neck1':        [3,24],\n",
    "        'Head':         [3,27],\n",
    "        'HeadEnd':      [3,30],\n",
    "\n",
    "        'RShoulder':    [3,33], \n",
    "        'RArm':         [3,36],\n",
    "        'RArm1':        [3,39],\n",
    "        'RHand':        [3,42],    \n",
    "        'RHandM1':      [3,45],\n",
    "        'RHandM2':      [3,48],\n",
    "        'RHandM3':      [3,51],\n",
    "        'RHandM4':      [3,54],\n",
    "\n",
    "        'RHandR':       [3,57],\n",
    "        'RHandR1':      [3,60],\n",
    "        'RHandR2':      [3,63],\n",
    "        'RHandR3':      [3,66],\n",
    "        'RHandR4':      [3,69],\n",
    "\n",
    "        'RHandP':       [3,72],\n",
    "        'RHandP1':      [3,75],\n",
    "        'RHandP2':      [3,78],\n",
    "        'RHandP3':      [3,81],\n",
    "        'RHandP4':      [3,84],\n",
    "\n",
    "        'RHandI':       [3,87],\n",
    "        'RHandI1':      [3,90],\n",
    "        'RHandI2':      [3,93],\n",
    "        'RHandI3':      [3,96],\n",
    "        'RHandI4':      [3,99],\n",
    "\n",
    "        'RHandT1':      [3,102],\n",
    "        'RHandT2':      [3,105],\n",
    "        'RHandT3':      [3,108],\n",
    "        'RHandT4':      [3,111],\n",
    "\n",
    "        'LShoulder':    [3,114], \n",
    "        'LArm':         [3,117],\n",
    "        'LArm1':        [3,120],\n",
    "        'LHand':        [3,123],    \n",
    "        'LHandM1':      [3,126],\n",
    "        'LHandM2':      [3,129],\n",
    "        'LHandM3':      [3,132],\n",
    "        'LHandM4':      [3,135],\n",
    "\n",
    "        'LHandR':       [3,138],\n",
    "        'LHandR1':      [3,141],\n",
    "        'LHandR2':      [3,144],\n",
    "        'LHandR3':      [3,147],\n",
    "        'LHandR4':      [3,150],\n",
    "\n",
    "        'LHandP':       [3,153],\n",
    "        'LHandP1':      [3,156],\n",
    "        'LHandP2':      [3,159],\n",
    "        'LHandP3':      [3,162],\n",
    "        'LHandP4':      [3,165],\n",
    "\n",
    "        'LHandI':       [3,168],\n",
    "        'LHandI1':      [3,171],\n",
    "        'LHandI2':      [3,174],\n",
    "        'LHandI3':      [3,177],\n",
    "        'LHandI4':      [3,180],\n",
    "\n",
    "        'LHandT1':      [3,183],\n",
    "        'LHandT2':      [3,186],\n",
    "        'LHandT3':      [3,189],\n",
    "        'LHandT4':      [3,192],\n",
    "\n",
    "        'RUpLeg':       [3,195],\n",
    "        'RLeg':         [3,198],\n",
    "        'RFoot':        [3,201],\n",
    "        'RFootF':       [3,204],\n",
    "        'RToeBase':     [3,207],\n",
    "        'RToeBaseEnd':  [3,210],\n",
    "\n",
    "        'LUpLeg':       [3,213],\n",
    "        'LLeg':         [3,216],\n",
    "        'LFoot':        [3,219],\n",
    "        'LFootF':       [3,222],\n",
    "        'LToeBase':     [3,225],\n",
    "        'LToeBaseEnd':  [3,228],\n",
    "        },\n",
    "\n",
    "    \"beat_full\" : {\n",
    "        'Hips':         6,\n",
    "        'Spine':        3,\n",
    "        'Spine1':       3,\n",
    "        'Spine2':       3,\n",
    "        'Spine3':       3,\n",
    "        'Neck':         3,\n",
    "        'Neck1':        3,\n",
    "        'Head':         3,\n",
    "        'HeadEnd':      3,\n",
    "\n",
    "        'RShoulder':    3,\n",
    "        'RArm':         3,\n",
    "        'RArm1':        3,\n",
    "        'RHand':        3,    \n",
    "        'RHandM1':      3,\n",
    "        'RHandM2':      3,\n",
    "        'RHandM3':      3,\n",
    "        'RHandM4':      3,\n",
    "\n",
    "        'RHandR':       3,\n",
    "        'RHandR1':      3,\n",
    "        'RHandR2':      3,\n",
    "        'RHandR3':      3,\n",
    "        'RHandR4':      3,\n",
    "\n",
    "        'RHandP':       3,\n",
    "        'RHandP1':      3,\n",
    "        'RHandP2':      3,\n",
    "        'RHandP3':      3,\n",
    "        'RHandP4':      3,\n",
    "\n",
    "        'RHandI':       3,\n",
    "        'RHandI1':      3,\n",
    "        'RHandI2':      3,\n",
    "        'RHandI3':      3,\n",
    "        'RHandI4':      3,\n",
    "\n",
    "        'RHandT1':      3,\n",
    "        'RHandT2':      3,\n",
    "        'RHandT3':      3,\n",
    "        'RHandT4':      3,\n",
    "\n",
    "        'LShoulder':    3, \n",
    "        'LArm':         3,\n",
    "        'LArm1':        3,\n",
    "        'LHand':        3,    \n",
    "        'LHandM1':      3,\n",
    "        'LHandM2':      3,\n",
    "        'LHandM3':      3,\n",
    "        'LHandM4':      3,\n",
    "\n",
    "        'LHandR':       3,\n",
    "        'LHandR1':      3,\n",
    "        'LHandR2':      3,\n",
    "        'LHandR3':      3,\n",
    "        'LHandR4':      3,\n",
    "\n",
    "        'LHandP':       3,\n",
    "        'LHandP1':      3,\n",
    "        'LHandP2':      3,\n",
    "        'LHandP3':      3,\n",
    "        'LHandP4':      3,\n",
    "\n",
    "        'LHandI':       3,\n",
    "        'LHandI1':      3,\n",
    "        'LHandI2':      3,\n",
    "        'LHandI3':      3,\n",
    "        'LHandI4':      3,\n",
    "\n",
    "        'LHandT1':      3,\n",
    "        'LHandT2':      3,\n",
    "        'LHandT3':      3,\n",
    "        'LHandT4':      3,\n",
    "\n",
    "        'RUpLeg':       3,\n",
    "        'RLeg':         3,\n",
    "        'RFoot':        3,\n",
    "        'RFootF':       3,\n",
    "        'RToeBase':     3,\n",
    "        'RToeBaseEnd':  3,\n",
    "\n",
    "        'LUpLeg':       3,\n",
    "        'LLeg':         3,\n",
    "        'LFoot':        3,\n",
    "        'LFootF':       3,\n",
    "        'LToeBase':     3,\n",
    "        'LToeBaseEnd':  3,\n",
    "    },\n",
    "    \n",
    "    \"beat_141\" : {\n",
    "            'Spine':       3 ,\n",
    "            'Neck':        3 ,\n",
    "            'Neck1':       3 ,\n",
    "            'RShoulder':   3 , \n",
    "            'RArm':        3 ,\n",
    "            'RArm1':       3 ,\n",
    "            'RHand':       3 ,    \n",
    "            'RHandM1':     3 ,\n",
    "            'RHandM2':     3 ,\n",
    "            'RHandM3':     3 ,\n",
    "            'RHandR':      3 ,\n",
    "            'RHandR1':     3 ,\n",
    "            'RHandR2':     3 ,\n",
    "            'RHandR3':     3 ,\n",
    "            'RHandP':      3 ,\n",
    "            'RHandP1':     3 ,\n",
    "            'RHandP2':     3 ,\n",
    "            'RHandP3':     3 ,\n",
    "            'RHandI':      3 ,\n",
    "            'RHandI1':     3 ,\n",
    "            'RHandI2':     3 ,\n",
    "            'RHandI3':     3 ,\n",
    "            'RHandT1':     3 ,\n",
    "            'RHandT2':     3 ,\n",
    "            'RHandT3':     3 ,\n",
    "            'LShoulder':   3 , \n",
    "            'LArm':        3 ,\n",
    "            'LArm1':       3 ,\n",
    "            'LHand':       3 ,    \n",
    "            'LHandM1':     3 ,\n",
    "            'LHandM2':     3 ,\n",
    "            'LHandM3':     3 ,\n",
    "            'LHandR':      3 ,\n",
    "            'LHandR1':     3 ,\n",
    "            'LHandR2':     3 ,\n",
    "            'LHandR3':     3 ,\n",
    "            'LHandP':      3 ,\n",
    "            'LHandP1':     3 ,\n",
    "            'LHandP2':     3 ,\n",
    "            'LHandP3':     3 ,\n",
    "            'LHandI':      3 ,\n",
    "            'LHandI1':     3 ,\n",
    "            'LHandI2':     3 ,\n",
    "            'LHandI3':     3 ,\n",
    "            'LHandT1':     3 ,\n",
    "            'LHandT2':     3 ,\n",
    "            'LHandT3':     3 ,\n",
    "        },\n",
    "    \n",
    "    \"beat_27\" : {\n",
    "            'Spine':       3 ,\n",
    "            'Neck':        3 ,\n",
    "            'Neck1':       3 ,\n",
    "            'RShoulder':   3 , \n",
    "            'RArm':        3 ,\n",
    "            'RArm1':       3 ,\n",
    "            'LShoulder':   3 , \n",
    "            'LArm':        3 ,\n",
    "            'LArm1':       3 ,     \n",
    "        },\n",
    "    \n",
    "    \"mixamo_joints\" : {\n",
    "        'Hips': [6, 6],\n",
    "        'Spine': [3, 9],\n",
    "        'Spine1': [3, 12],\n",
    "        'Spine2': [3, 15],\n",
    "        'LShoulder': [3, 18],\n",
    "        'LArm': [3, 21],\n",
    "        'LArm1': [3, 24],\n",
    "        'LHand': [3, 27],\n",
    "        'LHandI1': [3, 30],\n",
    "        'LHandI2': [3, 33],\n",
    "        'LHandI3': [3, 36],\n",
    "\n",
    "        'LHandM1': [3, 39],\n",
    "        'LHandM2': [3, 42],\n",
    "        'LHandM3': [3, 45],\n",
    "\n",
    "        'LHandP1': [3, 48],\n",
    "        'LHandP2': [3, 51],\n",
    "        'LHandP3': [3, 54],\n",
    "\n",
    "        'LHandR1': [3, 57],\n",
    "        'LHandR2': [3, 60],\n",
    "        'LHandR3': [3, 63],\n",
    "\n",
    "        'LHandT1': [3, 66],\n",
    "        'LHandT2': [3, 69],\n",
    "        'LHandT3': [3, 72],\n",
    "\n",
    "        'Neck': [3, 75],\n",
    "        'Head': [3, 78],\n",
    "\n",
    "        'RShoulder': [3, 81],\n",
    "        'RArm': [3, 84],\n",
    "        'RArm1': [3, 87],\n",
    "        'RHand': [3, 90],\n",
    "        'RHandI1': [3, 93],\n",
    "        'RHandI2': [3, 96],\n",
    "        'RHandI3': [3, 99],\n",
    "\n",
    "        'RHandM1': [3, 102],\n",
    "        'RHandM2': [3, 105],\n",
    "        'RHandM3': [3, 108],\n",
    "\n",
    "        'RHandP1': [3, 111],\n",
    "        'RHandP2': [3, 114],\n",
    "        'RHandP3': [3, 117],\n",
    "\n",
    "        'RHandR1': [3, 120],\n",
    "        'RHandR2': [3, 123],\n",
    "        'RHandR3': [3, 126],\n",
    "\n",
    "        'RHandT1': [3, 129],\n",
    "        'RHandT2': [3, 132],\n",
    "        'RHandT3': [3, 135],\n",
    "\n",
    "        'LUpLeg': [3, 138],\n",
    "        'LLeg': [3, 141],\n",
    "        'LFoot': [3, 144],\n",
    "        'LToeBase': [3, 147],\n",
    "\n",
    "        'RUpLeg': [3, 150],\n",
    "        'RLeg': [3, 153],\n",
    "        'RFoot': [3, 156],\n",
    "        'RToeBase': [3, 159]\n",
    "    },\n",
    "\n",
    "    \"mixamo_full\" : {\n",
    "        'Hips': 6,\n",
    "        'Spine': 3,\n",
    "        'Spine1': 3,\n",
    "        'Spine2': 3,\n",
    "        'LShoulder': 3,\n",
    "        'LArm': 3,\n",
    "        'LArm1': 3,\n",
    "        'LHand': 3,\n",
    "        'LHandI1': 3,\n",
    "        'LHandI2': 3,\n",
    "        'LHandI3': 3,\n",
    "    \n",
    "        'LHandM1': 3,\n",
    "        'LHandM2': 3,\n",
    "        'LHandM3': 3,\n",
    "    \n",
    "        'LHandP1': 3,\n",
    "        'LHandP2': 3,\n",
    "        'LHandP3': 3,\n",
    "    \n",
    "        'LHandR1': 3,\n",
    "        'LHandR2': 3,\n",
    "        'LHandR3': 3,\n",
    "    \n",
    "        'LHandT1': 3,\n",
    "        'LHandT2': 3,\n",
    "        'LHandT3': 3,\n",
    "    \n",
    "        'Neck': 3,\n",
    "        'Head': 3,\n",
    "    \n",
    "        'RShoulder': 3,\n",
    "        'RArm': 3,\n",
    "        'RArm1': 3,\n",
    "        'RHand': 3,\n",
    "        'RHandI1': 3,\n",
    "        'RHandI2': 3,\n",
    "        'RHandI3': 3,\n",
    "    \n",
    "        'RHandM1': 3,\n",
    "        'RHandM2': 3,\n",
    "        'RHandM3': 3,\n",
    "    \n",
    "        'RHandP1': 3,\n",
    "        'RHandP2': 3,\n",
    "        'RHandP3': 3,\n",
    "    \n",
    "        'RHandR1': 3,\n",
    "        'RHandR2': 3,\n",
    "        'RHandR3': 3,\n",
    "    \n",
    "        'RHandT1': 3,\n",
    "        'RHandT2': 3,\n",
    "        'RHandT3': 3,\n",
    "    \n",
    "        'LUpLeg': 3,\n",
    "        'LLeg': 3,\n",
    "        'LFoot': 3,\n",
    "        'LToeBase': 3,\n",
    "    \n",
    "        'RUpLeg': 3,\n",
    "        'RLeg': 3,\n",
    "        'RFoot': 3,\n",
    "        'RToeBase': 3\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_fps = 30\n",
    "ori_list = joint_list[\"mixamo_joints\"]\n",
    "target_list = joint_list[\"mixamo_full\"]\n",
    "ori_data_path = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_v0.2.1_partial2/\"\n",
    "ori_data_path_npy = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_npy_partial2/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data processing ...: 100% 1/1 [00:00<00:00, 228.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for speaker in tqdm(range(1,2), \"Data processing ...\"):\n",
    "    for f in os.listdir(ori_data_path+str(speaker)):\n",
    "        if(f.endswith('wav')):\n",
    "            audio_folder = os.path.join(ori_data_path_npy+str(speaker))\n",
    "            os.makedirs(audio_folder, exist_ok=True)\n",
    "            audioOut = os.path.join(ori_data_path_npy+str(speaker), f.replace('wav', 'npy'))\n",
    "            if(not os.path.exists(audioOut)):\n",
    "                a, sr = librosa.load(os.path.join(ori_data_path+str(speaker), f), sr=16000)\n",
    "                np.save(audioOut, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_fps: 30, reduce json 2, reduce bvh 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:14,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "#calculate mean and build cache for data. \n",
    "target_fps = 30\n",
    "ori_list = joint_list[\"mixamo_joints\"]\n",
    "target_list = joint_list[\"mixamo_joints\"]\n",
    "ori_data_path = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_v0.2.1_partial2/\"\n",
    "#wave cache from a = librosa.load(sr=16000) and np.save(a)\n",
    "ori_data_path_npy = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_npy_partial2/\"\n",
    "ori_data_path_ann = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_v0.2.1_partial2/\"\n",
    "cache_path = f\"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_{target_fps}_mixamo6/\"\n",
    "reduce_factor_json = int(60/target_fps)\n",
    "reduce_factor_bvh = int(120/target_fps)\n",
    "print(f\"target_fps: {target_fps}, reduce json {reduce_factor_json}, reduce bvh {reduce_factor_bvh}\")\n",
    "speakers = sorted(os.listdir(ori_data_path),key=str,)\n",
    "\n",
    "npy_s_v = []\n",
    "npy_s_k = []\n",
    "json_s_v = []\n",
    "bvh_s_v = []\n",
    "\n",
    "load_type = \"train\"\n",
    "if not os.path.exists(f\"{cache_path}\"): \n",
    "    os.mkdir(cache_path)\n",
    "if not os.path.exists(f\"{cache_path}{load_type}/\"): \n",
    "    os.mkdir(f\"{cache_path}{load_type}/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/wave16k/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/bvh_rot/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/bvh_full/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/bvh_rot_vis/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/facial52/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/text/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/emo/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/sem/\")     \n",
    "\n",
    "for speaker in [1]: #range(1, 31):#replace to 1, 31 for all speakers\n",
    "    if speaker in []:\n",
    "        print(\"Skip: \", speaker)\n",
    "        # break\n",
    "    else:\n",
    "        all_data = os.listdir(ori_data_path_npy+str(speaker))\n",
    "        npy_all = []\n",
    "        json_all = []\n",
    "        bvh_all = []   \n",
    "        for ii, file in tqdm(enumerate(all_data)):\n",
    "            file = file[:-4]\n",
    "            npy_all.extend(list(np.load(f\"{ori_data_path_npy}/{file.split('_')[0]}/{file}.npy\")))\n",
    "\n",
    "        npy_all = np.array(npy_all)\n",
    "        npy_mean = np.mean(npy_all, axis=0)\n",
    "        npy_std = np.std(npy_all, axis=0)\n",
    "        npy_s_v.append([npy_mean, npy_std])\n",
    "        npy_s_k.append([len(npy_all)/16000/60])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [08:58, 44.86s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Function to process a single speaker\n",
    "def process_speaker(speaker):\n",
    "    if speaker in []:\n",
    "        print(\"Skip: \", speaker)\n",
    "        return\n",
    "    \n",
    "    ori_data_path_npy = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_npy_partial2/\"\n",
    "    ori_data_path = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_v0.2.1_partial2/\"\n",
    "    ori_data_path_ann = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_v0.2.1_partial2/\"\n",
    "    cache_path = f\"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_{target_fps}_mixamo6/\"\n",
    "    load_type = \"train\"\n",
    "\n",
    "    all_data = os.listdir(f\"{ori_data_path_npy}{speaker}\")\n",
    "    npy_all = []\n",
    "    json_all = []\n",
    "    bvh_all = []   \n",
    "\n",
    "    for ii, file in tqdm(enumerate(all_data)):\n",
    "        file = file[:-4]\n",
    "        shutil.copy(f\"{ori_data_path_npy}/{file.split('_')[0]}/{file}.npy\", f\"{cache_path}{load_type}/wave16k/{file}.npy\")\n",
    "        try:\n",
    "            shutil.copy(f\"{ori_data_path}/{file.split('_')[0]}/{file}.TextGrid\", f\"{cache_path}{load_type}/text/{file}.TextGrid\")\n",
    "        except:\n",
    "            print(f\"{file}.TextGrid\")\n",
    "        try:\n",
    "            shutil.copy(f\"{ori_data_path_ann}/{file.split('_')[0]}/{file}.txt\", f\"{cache_path}{load_type}/sem/{file}.txt\")\n",
    "        except:\n",
    "            print(f\"{file}.txt\")\n",
    "        try:\n",
    "            shutil.copy(f\"{ori_data_path_ann}/{file.split('_')[0]}/{file}.csv\", f\"{cache_path}{load_type}/emo/{file}.csv\")\n",
    "        except:\n",
    "            print(f\"{file}.csv\")\n",
    "\n",
    "        npy_all.extend(list(np.load(f\"{ori_data_path_npy}/{file.split('_')[0]}/{file}.npy\")))\n",
    "\n",
    "        with open(f\"{ori_data_path}/{file.split('_')[0]}/{file}.json\", \"r\", encoding='utf-8') as json_file_raw:\n",
    "            json_file = json.load(json_file_raw)\n",
    "            with open(f\"{cache_path}{load_type}/facial52/{file}.json\", \"w\") as reduced_json:\n",
    "                counter = 0\n",
    "                new_frames_list = []\n",
    "                for json_data in json_file[\"frames\"]:\n",
    "                    json_all.append(json_data[\"weights\"])\n",
    "                    if counter % reduce_factor_json == 0:\n",
    "                        new_frames_list.append(json_data)\n",
    "                    counter += 1\n",
    "                json_new = {\"names\": json_file[\"names\"], \"frames\": new_frames_list}\n",
    "                json.dump(json_new, reduced_json)\n",
    "\n",
    "            with open(f\"{ori_data_path}/{file.split('_')[0]}/{file}.bvh\", \"r\") as bvh_file:\n",
    "                with open(f\"{cache_path}{load_type}/bvh_rot/{file}.bvh\", \"w\") as reduced_raw_bvh:\n",
    "                    with open(f\"{cache_path}{load_type}/bvh_full/{file}.bvh\", \"w\") as reduced_full_bvh:\n",
    "                        with open(f\"{cache_path}{load_type}/bvh_rot_vis/{file}.bvh\", \"w\") as reduced_trainable_bvh:\n",
    "                            for i, line_data in enumerate(bvh_file.readlines()):\n",
    "                                if i < 316: \n",
    "                                    reduced_full_bvh.write(line_data)\n",
    "                                    reduced_trainable_bvh.write(line_data)\n",
    "                                if i >= 316:\n",
    "                                    data = np.fromstring(line_data, dtype=float, sep=' ')\n",
    "                                    bvh_all.append(data)\n",
    "                                    if i % reduce_factor_bvh == 0:\n",
    "                                        reduced_full_bvh.write(line_data)\n",
    "                                        trainable_rotation = np.zeros_like(data)\n",
    "                                        for k, v in target_list.items():\n",
    "                                            trainable_rotation[ori_list[k][1] - v:ori_list[k][1]] = data[ori_list[k][1] - v:ori_list[k][1]]\n",
    "\n",
    "                                        trainable_line_data = np.array2string(trainable_rotation, max_line_width=np.inf, precision=6, suppress_small=False, separator=' ')\n",
    "                                        reduced_trainable_bvh.write(trainable_line_data[1:-2] + \"\\n\")\n",
    "                                        data_rotation = np.zeros((1))   \n",
    "                                        for k, v in target_list.items():\n",
    "                                            data_rotation = np.concatenate((data_rotation, data[ori_list[k][1] - v:ori_list[k][1]]))                             \n",
    "                                            raw_line_data = np.array2string(data_rotation[1:], max_line_width=np.inf, precision=6, suppress_small=False, separator=' ')\n",
    "                                        reduced_raw_bvh.write(raw_line_data[1:-2] + \"\\n\")\n",
    "\n",
    "    npy_all = np.array(npy_all)\n",
    "    npy_mean = np.mean(npy_all, axis=0)\n",
    "    npy_std = np.std(npy_all, axis=0)\n",
    "    np.save(f\"{cache_path}{load_type}/wave16k/npy_mean_{speaker}.npy\", npy_mean)\n",
    "    np.save(f\"{cache_path}{load_type}/wave16k/npy_std_{speaker}.npy\", npy_std)  \n",
    "\n",
    "    json_all = np.array(json_all)\n",
    "    json_mean = np.mean(json_all, axis=0)\n",
    "    json_std = np.std(json_all, axis=0)\n",
    "    np.save(f\"{cache_path}{load_type}/facial52/json_mean_{speaker}.npy\", json_mean)\n",
    "    np.save(f\"{cache_path}{load_type}/facial52/json_std_{speaker}.npy\", json_std)\n",
    "\n",
    "    bvh_all = np.array(bvh_all)\n",
    "    bvh_mean = np.mean(bvh_all, axis=0)\n",
    "    bvh_std = np.std(bvh_all, axis=0)\n",
    "    data_rotation = np.zeros((1))\n",
    "    for k, v in target_list.items():\n",
    "        data_rotation = np.concatenate((data_rotation, bvh_mean[ori_list[k][1] - v:ori_list[k][1]]))\n",
    "    new_npy_mean = data_rotation[1:]\n",
    "    data_rotation = np.zeros((1))\n",
    "    for k, v in target_list.items():\n",
    "        data_rotation = np.concatenate((data_rotation, bvh_std[ori_list[k][1] - v:ori_list[k][1]]))\n",
    "    new_npy_std = data_rotation[1:]\n",
    "    np.save(f\"{cache_path}{load_type}/bvh_rot/bvh_mean_{speaker}.npy\", new_npy_mean)\n",
    "    np.save(f\"{cache_path}{load_type}/bvh_rot/bvh_std_{speaker}.npy\", new_npy_std)\n",
    "\n",
    "# Set up multiprocessing pool\n",
    "if __name__ == \"__main__\":\n",
    "    target_fps = 30\n",
    "    ori_list = joint_list[\"mixamo_joints\"]\n",
    "    target_list = joint_list[\"mixamo_full\"]\n",
    "    reduce_factor_json = int(60 / target_fps)\n",
    "    reduce_factor_bvh = int(120 / target_fps)\n",
    "    cache_path = f\"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_{target_fps}_mixamo6/\"\n",
    "    \n",
    "    # Create required directories if they don't exist\n",
    "    load_type = \"train\"\n",
    "    if not os.path.exists(f\"{cache_path}{load_type}/\"):\n",
    "        os.makedirs(f\"{cache_path}{load_type}/wave16k/\")\n",
    "        os.makedirs(f\"{cache_path}{load_type}/bvh_rot/\")\n",
    "        os.makedirs(f\"{cache_path}{load_type}/bvh_full/\")\n",
    "        os.makedirs(f\"{cache_path}{load_type}/bvh_rot_vis/\")\n",
    "        os.makedirs(f\"{cache_path}{load_type}/facial52/\")\n",
    "        os.makedirs(f\"{cache_path}{load_type}/text/\")\n",
    "        os.makedirs(f\"{cache_path}{load_type}/emo/\")\n",
    "        os.makedirs(f\"{cache_path}{load_type}/sem/\")\n",
    "\n",
    "    # List of speakers to process\n",
    "    speakers = list(range(1, 2))\n",
    "\n",
    "    # Create a pool of workers equal to the number of CPUs\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        pool.map(process_speaker, speakers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = f\"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_{target_fps}_mixamo6/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[13.583333333333334]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npy_s_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18755722\n",
      " audio mean: -1.533552131149918e-05, std: 0.11750699579715729\n",
      "[7.05451693e-02 7.03765864e-02 5.55606167e-01 7.13724240e-01\n",
      " 7.13825507e-01 7.30097568e-03 1.25667172e-01 1.24597327e-01\n",
      " 2.09337054e-02 2.13082851e-02 6.43908944e-03 6.52360686e-03\n",
      " 3.30387944e-01 4.41611129e-03 7.39194859e-04 3.30778832e-01\n",
      " 2.63136211e-01 2.67653520e-01 6.34216116e-02 6.33497450e-02\n",
      " 1.76891711e-01 1.76920544e-01 5.83773975e-02 4.40580928e-03\n",
      " 1.82119109e-01 1.35550601e-02 1.35477064e-01 4.22890636e-02\n",
      " 4.03363728e-02 3.99690877e-02 4.43234250e-02 3.05983329e-01\n",
      " 1.90806897e-03 3.35903770e-01 3.38112221e-01 1.08650603e-02\n",
      " 1.24035169e-02 2.67550208e-02 1.69665602e-03 1.68684369e-01\n",
      " 1.15747320e-01 2.40894488e-02 7.65906285e-02 9.96368023e-01\n",
      " 9.72321205e-01 4.77366419e-02 6.15470697e-02 6.57291685e-02\n",
      " 6.86528965e-02 8.43922932e-02 9.35055864e-02]\n",
      " json mean: [2.74052114e-02 2.73530956e-02 3.92775515e-01 2.95098140e-01\n",
      " 2.95280231e-01 2.94794111e-02 2.34159969e-01 2.42457223e-01\n",
      " 2.52481095e-03 2.55462249e-03 4.98790740e-03 5.03254117e-03\n",
      " 2.91118648e-01 3.54982076e-03 4.45004649e-04 1.96339709e-01\n",
      " 2.08950228e-01 2.10650644e-01 2.83170992e-01 2.83183247e-01\n",
      " 6.51983640e-01 6.51982321e-01 1.77862071e-01 1.03925556e-02\n",
      " 2.62137705e-01 1.93777804e-02 1.55559702e-01 1.27150260e-01\n",
      " 1.24544930e-01 1.42082142e-02 1.66831679e-02 2.58524776e-01\n",
      " 8.73728082e-03 4.36100837e-01 4.34509008e-01 5.42195535e-02\n",
      " 5.58465126e-02 6.57626290e-02 4.78300485e-03 1.01152457e-01\n",
      " 8.06686390e-02 4.48671920e-02 2.39031481e-01 3.51835923e-01\n",
      " 3.22233592e-01 2.97159891e-01 3.17563124e-01 7.95865080e-02\n",
      " 8.27592628e-02 1.23869472e-01 1.36267915e-01], std: [0.07206601 0.07197985 0.20224619 0.22922503 0.22924129 0.02318394\n",
      " 0.09618509 0.09577478 0.03925725 0.03960692 0.02177254 0.02191496\n",
      " 0.15595845 0.01803088 0.00737694 0.15605068 0.1391833  0.14037291\n",
      " 0.06833064 0.06829191 0.11411706 0.11412636 0.06555702 0.01800983\n",
      " 0.11579094 0.03158985 0.09986877 0.055797   0.05449357 0.0542449\n",
      " 0.05712332 0.15008791 0.01185206 0.15725493 0.15777103 0.02828218\n",
      " 0.03021825 0.04438124 0.01117619 0.11143824 0.09231075 0.04211243\n",
      " 0.07509044 0.2708362  0.26754799 0.05928199 0.06731324 0.06956261\n",
      " 0.0710929  0.07882214 0.08296895]\n",
      "\n",
      "\n",
      "[ 5.34374880e+00  8.76051665e+01  1.59148233e+01 -5.88865893e-01\n",
      " -2.59581070e+00 -6.18497410e-01  8.31039813e-01  4.76828871e-01\n",
      "  1.05059925e-01  8.31169451e-01  4.76984965e-01  1.05083197e-01\n",
      "  8.33538331e-01  4.78313978e-01  1.05393360e-01 -1.42494160e+02\n",
      "  8.78066920e+01  4.59425380e+01 -1.79425853e+01  8.00262774e+01\n",
      " -2.20387883e+01  1.00390224e+01  2.13801123e+01  4.59829551e+01\n",
      "  9.24896039e+00 -1.16609374e+01  2.18019027e+01  2.34635398e+01\n",
      "  1.24902319e+01 -3.74472573e+00  4.93960821e-07  2.02417300e+01\n",
      "  2.29353453e-05 -1.69164665e-08  1.35026936e+01 -8.03154921e-04\n",
      "  1.40487444e+01  1.11405383e+01 -1.31403791e+01  1.14862807e-06\n",
      "  3.05235494e+01  9.61785702e-05  9.80833643e-05  2.03806360e+01\n",
      "  9.19173123e-05 -1.70854780e+01  8.96802917e+00 -3.34931333e+01\n",
      " -2.05343912e-04  3.03043411e+01  3.92226884e-04 -7.78411205e-05\n",
      "  2.02317587e+01 -5.96053388e-04 -8.34664690e+00  8.16344949e+00\n",
      " -2.43809649e+01  5.68393274e-07  3.31832160e+01  1.38207531e-06\n",
      "  3.55245796e-07  2.21585948e+01 -1.55650100e-04  5.24355373e+01\n",
      "  2.72637412e+01  4.72764227e+01  1.58394318e+00 -1.58738302e+01\n",
      " -2.59273783e+01 -2.45494275e-01  3.26085740e+00 -3.29663798e+00\n",
      " -5.29180871e-02  2.29662990e-01  6.66402240e-02 -5.29180465e-02\n",
      "  2.29662973e-01  6.66402138e-02  1.78052414e+02  8.84256814e+01\n",
      " -4.46505973e+01 -3.32293782e+01  7.77074945e+01 -3.40762838e+01\n",
      " -8.32008892e+00  1.98124017e+01 -4.88417630e+01 -1.63705353e+01\n",
      " -1.11306116e+01 -2.17960500e+01  1.14696866e+00  2.62720434e+01\n",
      "  1.26276768e+01 -6.51791454e-06  1.85701247e+01 -6.92069561e-05\n",
      " -1.99106811e-06  1.23869036e+01  2.06485773e-04  1.15005938e+01\n",
      "  2.46468141e+01  2.41354517e+01  3.56599114e-06  3.34655440e+01\n",
      " -3.35402781e-05  2.61697737e-06  2.23516077e+01 -1.99326725e-05\n",
      "  3.95578797e+01  7.94915833e+00  4.41312375e+01 -4.63172852e-06\n",
      "  2.66796805e+01  6.76100416e-05 -3.95337822e-05  1.78053939e+01\n",
      "  2.00906723e-04  2.63415512e+01  1.99852297e+01  3.47296125e+01\n",
      "  4.66556146e-06  2.99753695e+01 -4.06587272e-05  6.64817133e-07\n",
      "  2.00119150e+01 -1.45244781e-05 -2.81817401e+01  3.13406468e+01\n",
      " -3.17046151e+01  1.37760680e+00 -8.94058372e+00  2.97720232e+01\n",
      "  5.14126173e-01  7.25100386e+00  7.32728981e+00  1.23429069e+01\n",
      "  1.01000593e+01 -1.40350895e+02 -1.74422468e-01 -9.79301998e+00\n",
      "  1.46457438e-01  1.39742385e+01  7.96468241e+01 -7.78412529e+00\n",
      "  0.00000000e+00  2.57273176e+01  0.00000000e+00 -6.26784189e+00\n",
      "  1.29760491e+01  1.34350553e+02  7.95041006e-02 -4.83940511e+00\n",
      " -1.09015651e-01  3.69963363e+01  7.52602660e+01  2.45795743e+01\n",
      "  0.00000000e+00  2.60959101e+01 -1.00000000e-04] (159,)\n",
      "[5.89802734e+00 2.66789003e+00 1.25785493e+01 3.13834172e+00\n",
      " 9.91631001e-01 1.02370723e+00 3.67443048e-01 6.97743986e-01\n",
      " 3.55469347e-01 3.67560989e-01 6.97911107e-01 3.55549370e-01\n",
      " 3.68546697e-01 6.99837385e-01 3.56539883e-01 1.51517495e+02\n",
      " 2.10605143e+00 4.36955914e+01 8.72965403e+01 7.01829391e+00\n",
      " 5.69336507e+01 2.22970060e+01 2.14524035e+01 3.94070115e+01\n",
      " 2.51503765e+01 1.97415338e+01 2.01725880e+01 2.17323387e+00\n",
      " 1.62798284e+01 4.86135931e+00 1.56595671e-04 5.43390164e+00\n",
      " 4.71545884e-04 8.45551602e-05 3.62931495e+00 3.03982253e-04\n",
      " 3.25180870e+00 1.93857357e+01 3.58189219e+00 1.44438930e-04\n",
      " 7.35286146e+00 3.38933793e-04 7.71411275e-05 4.92242811e+00\n",
      " 2.15002102e-04 7.02058060e+00 1.46305800e+01 6.19686235e+00\n",
      " 2.37920680e-04 4.49409620e+00 4.76568363e-04 1.30154595e-04\n",
      " 3.00711418e+00 2.94040607e-04 6.07583217e+00 1.71768244e+01\n",
      " 5.80106871e+00 9.57500049e-05 5.45049853e+00 1.86576138e-04\n",
      " 4.61152032e-05 3.65077538e+00 1.23244713e-04 3.50876140e+00\n",
      " 2.50185077e+00 2.59157833e+00 2.91682164e+00 5.49293756e+00\n",
      " 7.27586922e+00 3.28393656e-01 4.12921773e+00 4.18099673e+00\n",
      " 1.45535166e+00 1.67180513e+00 9.60646141e-01 1.45535194e+00\n",
      " 1.67180538e+00 9.60646241e-01 8.86131819e+01 2.04239609e+00\n",
      " 8.87815919e+01 5.60673454e+01 6.49133510e+00 3.78280622e+01\n",
      " 2.08040642e+01 1.96363181e+01 4.12541541e+01 2.00327189e+01\n",
      " 1.80533857e+01 1.29216655e+01 2.46847832e+00 8.53409081e+00\n",
      " 3.88217557e+00 2.02990409e-04 5.27597388e+00 6.36116501e-04\n",
      " 1.09739505e-04 3.52236179e+00 4.12041602e-04 5.28812502e+00\n",
      " 1.03773658e+01 3.74941518e+00 7.65976084e-04 7.81177854e+00\n",
      " 1.25922848e-03 3.73761902e-04 5.23245174e+00 7.35864449e-04\n",
      " 8.43747443e+00 9.91929384e+00 5.15426191e+00 1.57144900e-04\n",
      " 3.59026455e+00 3.62303573e-04 9.56694653e-05 2.40090520e+00\n",
      " 2.26811609e-04 7.29443029e+00 9.69849708e+00 5.54091843e+00\n",
      " 2.29750628e-04 6.57766313e+00 5.51208687e-04 1.19293226e-04\n",
      " 4.40308012e+00 3.48588357e-04 3.58890180e+00 1.73304794e+00\n",
      " 2.75328076e+00 1.80829925e+00 4.26547547e+00 4.38177723e+00\n",
      " 2.82381234e-01 2.33053190e+00 2.38390238e+00 4.35179137e+00\n",
      " 7.42461109e+00 1.07616623e+02 1.98819693e-01 7.29581038e+00\n",
      " 7.73712121e-02 1.20566651e+02 6.05232348e+00 5.50130108e+01\n",
      " 0.00000000e+00 2.60399194e-01 0.00000000e+00 5.51511443e+00\n",
      " 8.24827048e+00 1.14481689e+02 1.34043964e-01 5.99702653e+00\n",
      " 5.56028138e-02 2.97518893e+01 4.83133828e+00 2.93716876e+01\n",
      " 0.00000000e+00 5.00310695e-01 7.41323235e-18] (159,)\n"
     ]
    }
   ],
   "source": [
    "npy_s_v = []\n",
    "json_s_v = []\n",
    "bvh_s_v = []\n",
    "\n",
    "npy_path = f\"{cache_path}{load_type}/wave16k/\"\n",
    "bvh_path = f\"{cache_path}{load_type}/bvh_rot/\"\n",
    "json_path = f\"{cache_path}{load_type}/facial52/\" \n",
    "        \n",
    "for i in [1]: #range(1, 31):\n",
    "    npy_s_v.append([np.load(f\"{npy_path}npy_mean_{i}.npy\"), np.load(f\"{npy_path}npy_std_{i}.npy\")])\n",
    "    json_s_v.append([np.load(f\"{json_path}json_mean_{i}.npy\"), np.load(f\"{json_path}json_std_{i}.npy\")])\n",
    "    bvh_s_v.append([np.load(f\"{bvh_path}bvh_mean_{i}.npy\"), np.load(f\"{bvh_path}bvh_std_{i}.npy\")])\n",
    "\n",
    "all_length = 0\n",
    "new_m = np.zeros_like(npy_s_v[0][0])\n",
    "new_s = np.zeros_like(npy_s_v[0][0])\n",
    "for i, (m, s) in enumerate(npy_s_v):\n",
    "    all_length += npy_s_k[i][0]\n",
    "    new_m += npy_s_k[i][0] * m\n",
    "new_m /= all_length\n",
    "for i, (m, s) in enumerate(npy_s_v):\n",
    "    new_s += ((s**2) + (m-new_m)**2) * npy_s_k[i][0]\n",
    "\n",
    "print(new_s)\n",
    "new_s /= all_length\n",
    "new_s = np.sqrt(new_s)\n",
    "print(f\" audio mean: {new_m}, std: {new_s}\") \n",
    "np.save(f\"{npy_path}npy_mean.npy\", new_m)\n",
    "np.save(f\"{npy_path}/npy_std.npy\", new_s)  \n",
    "\n",
    "new_m = np.zeros_like(json_s_v[0][0])\n",
    "new_s = np.zeros_like(json_s_v[0][0])\n",
    "all_length = 0\n",
    "for i, (m, s) in enumerate(json_s_v):\n",
    "    all_length += npy_s_k[i][0]\n",
    "    new_m += npy_s_k[i][0] * m\n",
    "new_m /= all_length\n",
    "for i, (m, s) in enumerate(json_s_v):\n",
    "    new_s += ((s**2) + (m-new_m)**2) * npy_s_k[i][0]\n",
    "\n",
    "print(new_s)\n",
    "new_s /= all_length\n",
    "new_s = np.sqrt(new_s)\n",
    "print(f\" json mean: {new_m}, std: {new_s}\") \n",
    "np.save(f\"{json_path}json_mean.npy\", new_m)\n",
    "np.save(f\"{json_path}/json_std.npy\", new_s)\n",
    "\n",
    "new_m = np.zeros_like(bvh_s_v[0][0])\n",
    "new_s = np.zeros_like(bvh_s_v[0][0])\n",
    "all_length = 0\n",
    "for i, (m, s) in enumerate(bvh_s_v):\n",
    "    all_length += npy_s_k[i][0]\n",
    "    new_m += npy_s_k[i][0] * m\n",
    "new_m /= all_length\n",
    "for i, (m, s) in enumerate(bvh_s_v):\n",
    "    new_s += ((s**2) + (m-new_m)**2) * npy_s_k[i][0]\n",
    "new_s /= all_length\n",
    "new_s = np.sqrt(new_s)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(new_m, new_m.shape)\n",
    "print(new_s, new_s.shape)\n",
    "np.save(f\"{bvh_path}bvh_mean.npy\", new_m)\n",
    "np.save(f\"{bvh_path}/bvh_std.npy\", new_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_rule_english = {\n",
    "    # 4h speakers x 10\n",
    "    \"1, 2, 3, 4, 6, 7, 8, 9, 11\":{\n",
    "        # 48+40+100=188mins each\n",
    "        \"train\": [\n",
    "            \"0_9_9\", \"0_10_10\", \"0_11_11\", \"0_12_12\", \"0_13_13\", \"0_14_14\", \"0_15_15\", \"0_16_16\", \\\n",
    "            \"0_17_17\", \"0_18_18\", \"0_19_19\", \"0_20_20\", \"0_21_21\", \"0_22_22\", \"0_23_23\", \"0_24_24\", \\\n",
    "            \"0_25_25\", \"0_26_26\", \"0_27_27\", \"0_28_28\", \"0_29_29\", \"0_30_30\", \"0_31_31\", \"0_32_32\", \\\n",
    "            \"0_33_33\", \"0_34_34\", \"0_35_35\", \"0_36_36\", \"0_37_37\", \"0_38_38\", \"0_39_39\", \"0_40_40\", \\\n",
    "            \"0_41_41\", \"0_42_42\", \"0_43_43\", \"0_44_44\", \"0_45_45\", \"0_46_46\", \"0_47_47\", \"0_48_48\", \\\n",
    "            \"0_49_49\", \"0_50_50\", \"0_51_51\", \"0_52_52\", \"0_53_53\", \"0_54_54\", \"0_55_55\", \"0_56_56\", \\\n",
    "            \n",
    "            \"0_66_66\", \"0_67_67\", \"0_68_68\", \"0_69_69\", \"0_70_70\", \"0_71_71\",  \\\n",
    "            \"0_74_74\", \"0_75_75\", \"0_76_76\", \"0_77_77\", \"0_78_78\", \"0_79_79\",  \\\n",
    "            \"0_82_82\", \"0_83_83\", \"0_84_84\", \"0_85_85\",  \\\n",
    "            \"0_88_88\", \"0_89_89\", \"0_90_90\", \"0_91_91\", \"0_92_92\", \"0_93_93\",  \\\n",
    "            \"0_96_96\", \"0_97_97\", \"0_98_98\", \"0_99_99\", \"0_100_100\", \"0_101_101\",  \\\n",
    "            \"0_104_104\", \"0_105_105\", \"0_106_106\", \"0_107_107\", \"0_108_108\", \"0_109_109\",  \\\n",
    "            \"0_112_112\", \"0_113_113\", \"0_114_114\", \"0_115_115\", \"0_116_116\", \"0_117_117\",  \\\n",
    "            \n",
    "            \"1_2_2\", \"1_3_3\", \"1_4_4\", \"1_5_5\", \"1_6_6\", \"1_7_7\", \"1_8_8\", \"1_9_9\", \"1_10_10\", \"1_11_11\",\n",
    "        ],\n",
    "        # 8+7+10=25mins each\n",
    "        \"val\": [\n",
    "            \"0_57_57\", \"0_58_58\", \"0_59_59\", \"0_60_60\", \"0_61_61\", \"0_62_62\", \"0_63_63\", \"0_64_64\", \\\n",
    "            \"0_72_72\", \"0_80_80\", \"0_86_86\", \"0_94_94\", \"0_102_102\", \"0_110_110\", \"0_118_118\", \\\n",
    "            \"1_12_12\",\n",
    "        ],\n",
    "        # 8+7+10=25mins each\n",
    "        \"test\": [\n",
    "           \"0_1_1\", \"0_2_2\", \"0_3_3\", \"0_4_4\", \"0_5_5\", \"0_6_6\", \"0_7_7\", \"0_8_8\", \\\n",
    "           \"0_65_65\", \"0_73_73\", \"0_81_81\", \"0_87_87\", \"0_95_95\", \"0_103_103\", \"0_111_111\", \\\n",
    "           \"1_1_1\",\n",
    "        ],\n",
    "    },\n",
    "    \n",
    "    # 1h speakers x 20\n",
    "    \"5, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30\":{\n",
    "        # 8+7+20=35mins each\n",
    "        \"train\": [\n",
    "            \"0_9_9\", \"0_10_10\", \"0_11_11\", \"0_12_12\", \"0_13_13\", \"0_14_14\", \"0_15_15\", \"0_16_16\", \\\n",
    "            \"0_66_66\", \"0_74_74\", \"0_82_82\", \"0_88_88\", \"0_96_96\", \"0_104_104\", \"0_112_112\", \"0_118_118\", \\\n",
    "            \"1_2_2\", \"1_3_3\", \n",
    "            \"1_0_0\", \"1_4_4\", # for speaker 29 only\n",
    "        ],\n",
    "        # 4+3.5+5 = 12.5mins each\n",
    "        # 0_65_a and 0_65_b denote the frist and second half of sequence 0_65_65\n",
    "        \"val\": [\n",
    "            \"0_5_5\", \"0_6_6\", \"0_7_7\", \"0_8_8\",  \\\n",
    "            \"0_65_b\", \"0_73_b\", \"0_81_b\", \"0_87_b\", \"0_95_b\", \"0_103_b\", \"0_111_b\", \\\n",
    "            \"1_1_b\",\n",
    "        ],\n",
    "        # 4+3.5+5 = 12.5mins each\n",
    "        \"test\": [\n",
    "           \"0_1_1\", \"0_2_2\", \"0_3_3\", \"0_4_4\", \\\n",
    "           \"0_65_a\", \"0_73_a\", \"0_81_a\", \"0_87_a\", \"0_95_a\", \"0_103_a\", \"0_111_a\", \\\n",
    "           \"1_1_a\",\n",
    "        ],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_rule_english = {\n",
    "    # 4h speakers x 10\n",
    "    \"1, 2, 3, 4, 6, 7, 8, 9, 11\":{\n",
    "        # 48+40+100=188mins each\n",
    "        # 80\n",
    "        \"train\": [\n",
    "            \"0_9_9\", \"0_10_10\", \"0_11_11\", \"0_12_12\", \"0_3_3\", \"0_4_4\", \"0_5_5\", \"0_6_6\",\n",
    "        ],\n",
    "        # 8+7+10=25mins each\n",
    "        # 25\n",
    "        \"val\": [\n",
    "            \"0_7_7\", \"0_8_8\",\n",
    "        ],\n",
    "        # 8+7+10=25mins each\n",
    "        # 25\n",
    "        \"test\": [\n",
    "           \"0_1_1\", \"0_2_2\",\n",
    "        ],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_sequence(source_path, save_path_a, save_path_b, file_id, fps = 15, sr = 16000, tmp=\"/data/nas07/PersonalData/danh/PantoMatrix/beat_tmp7/\"):\n",
    "    if not os.path.exists(tmp): os.mkdir(tmp)\n",
    "    cut_point = 30 if file_id.split(\"_\")[0] == \"0\" else 300 #in seconds\n",
    "    if source_path.endswith(\".npy\"):\n",
    "        data = np.load(source_path)\n",
    "        data_a = data[:sr*cut_point]\n",
    "        data_b = data[sr*cut_point:]\n",
    "        np.save(save_path_a, data_a)\n",
    "        np.save(save_path_b, data_b)\n",
    "        \n",
    "    elif source_path.endswith(\".bvh\"):\n",
    "        copy_lines = 316 if \"full\" in source_path or \"vis\" in source_path else 0\n",
    "        with open(source_path, \"r\") as data:\n",
    "            with open(save_path_a, \"w\") as data_a:\n",
    "                with open(save_path_b, \"w\") as data_b:\n",
    "                    for i, line_data in enumerate(data.readlines()):\n",
    "                        if i < copy_lines:\n",
    "                            data_a.write(line_data)\n",
    "                            data_b.write(line_data)\n",
    "                        elif i < cut_point * fps:\n",
    "                            data_a.write(line_data)\n",
    "                        else:\n",
    "                            data_b.write(line_data)\n",
    "    \n",
    "    elif source_path.endswith(\".json\"):\n",
    "        with open(source_path, \"r\", encoding='utf-8') as data:\n",
    "            json_file = json.load(data)\n",
    "            with open(save_path_a, \"w\") as data_a:\n",
    "                with open(save_path_b, \"w\") as data_b:\n",
    "                    new_frames_a = []\n",
    "                    new_frames_b = []\n",
    "                    for json_data in json_file[\"frames\"]:\n",
    "                        if json_data[\"time\"] < cut_point:\n",
    "                            new_frames_a.append(json_data)\n",
    "                        else:\n",
    "                            new_frame = json_data.copy()\n",
    "                            new_frame[\"time\"]-=cut_point\n",
    "                            new_frames_b.append(new_frame)\n",
    "                    json_new_a = {\"names\":json_file[\"names\"], \"frames\": new_frames_a}\n",
    "                    json_new_b = {\"names\":json_file[\"names\"], \"frames\": new_frames_b}\n",
    "                    json.dump(json_new_a, data_a)\n",
    "                    json.dump(json_new_b, data_b) \n",
    "        \n",
    "    else:\n",
    "        # processing in the dataloader\n",
    "        shutil.copy(source_path, save_path_a)\n",
    "        shutil.copy(source_path, save_path_b)\n",
    "    try:\n",
    "        shutil.move(source_path, tmp)\n",
    "    except:\n",
    "        print(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'default_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m os.listdir(default_path+\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m+folder)\n",
      "\u001b[31mNameError\u001b[39m: name 'default_path' is not defined"
     ]
    }
   ],
   "source": [
    "os.listdir(default_path+\"/\"+folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mroot\u001b[49m\u001b[38;5;241m/\u001b[39mnguyen\u001b[38;5;241m/\u001b[39mresearch\u001b[38;5;241m/\u001b[39mPantoMatrix\u001b[38;5;241m/\u001b[39mbeat_4english_30_full\u001b[38;5;241m/\u001b[39mtrain_copy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'root' is not defined"
     ]
    }
   ],
   "source": [
    "/root/nguyen/research/PantoMatrix/beat_4english_30_full/train_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_30_mixamo6/train_copy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'bvh_rot_vis',\n",
       " 'sem',\n",
       " 'facial52',\n",
       " 'bvh_full',\n",
       " 'wave16k',\n",
       " 'bvh_rot',\n",
       " 'emo']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = os.listdir(default_path)\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'npy'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(default_path+\"/\"+folders[5])[7].split(\".\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_30_mixamo6/train_copy'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_30_mixamo6/train_copy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'bvh_rot_vis',\n",
       " 'sem',\n",
       " 'facial52',\n",
       " 'bvh_full',\n",
       " 'wave16k',\n",
       " 'bvh_rot',\n",
       " 'emo']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(default_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1/1 [00:00<00:00, 51.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_30_mixamo6/train_copy/facial52/1_wayne_0_7_7.npy\n",
      "/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_30_mixamo6/train_copy/facial52/1_wayne_0_8_8.npy\n",
      "/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_30_mixamo6/train_copy/facial52/1_wayne_0_1_1.npy\n",
      "/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_30_mixamo6/train_copy/facial52/1_wayne_0_2_2.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# spilt data\n",
    "speaker_names = [\n",
    "    \"wayne\", \"scott\", \"solomon\", \"lawrence\", \"stewart\", \"carla\", \"sophie\", \"catherine\", \"miranda\", \"kieks\", \\\n",
    "    \"nidal\", \"zhao\", \"lu\", \"zhang\", \"carlos\", \"jorge\", \"itoi\", \"daiki\", \"jaime\", \"li\", \\\n",
    "    \"ayana\", \"luqi\", \"hailing\", \"kexin\", \"goto\", \"reamey\", \"yingqing\", \"tiffnay\", \"hanieh\", \"katya\",\n",
    "]\n",
    "default_path = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_30_mixamo6/train_copy\"\n",
    "four_hour_speakers = \"1, 2, 3, 4, 6, 7, 8, 9, 11\".split(\", \")\n",
    "one_hour_speakers = \"5, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30\".split(\", \")\n",
    "folders = os.listdir(default_path)\n",
    "if not os.path.exists(default_path.replace(\"train\", \"val\")): os.mkdir(default_path.replace(\"train\", \"val\"))\n",
    "if not os.path.exists(default_path.replace(\"train\", \"test\")): os.mkdir(default_path.replace(\"train\", \"test\"))\n",
    "endwith = []\n",
    "for folder in folders:\n",
    "    # print(os.listdir(default_path+\"/\"+folder))\n",
    "    # print(\"\\n\\n\")\n",
    "    # print(f\"folder: {(os.listdir(default_path + '/' + folder))[0]}\")\n",
    "    if not os.path.exists(default_path.replace(\"train\", \"val\")+\"/\"+folder): os.mkdir(default_path.replace(\"train\", \"val\")+\"/\"+folder)\n",
    "    if not os.path.exists(default_path.replace(\"train\", \"test\")+\"/\"+folder): os.mkdir(default_path.replace(\"train\", \"test\")+\"/\"+folder)\n",
    "    endwith.append(os.listdir(default_path+\"/\"+folder)[0].split(\".\")[-1])\n",
    "    \n",
    "for speaker_id in tqdm([1]):\n",
    "    val = split_rule_english[\"5, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30\"][\"val\"] if str(speaker_id) in one_hour_speakers else split_rule_english[\"1, 2, 3, 4, 6, 7, 8, 9, 11\"][\"val\"]\n",
    "    test = split_rule_english[\"5, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30\"][\"test\"] if str(speaker_id) in one_hour_speakers else split_rule_english[\"1, 2, 3, 4, 6, 7, 8, 9, 11\"][\"test\"]\n",
    "    for file_id in val:\n",
    "        for ide, folder in enumerate(folders):\n",
    "            if \"b\" in file_id:\n",
    "                print(f\"DATA: {default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id.split('_')[0]}_{file_id.split('_')[1]}_{file_id.split('_')[1]}.{endwith[ide]}\")\n",
    "                cut_sequence(\n",
    "                    source_path=f\"{default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id.split('_')[0]}_{file_id.split('_')[1]}_{file_id.split('_')[1]}.{endwith[ide]}\",\n",
    "                    save_path_a=f\"{default_path.replace('train', 'test')}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id.split('_')[0]}_{file_id.split('_')[1]}_a.{endwith[ide]}\",\n",
    "                    save_path_b=f\"{default_path.replace('train', 'val')}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id.split('_')[0]}_{file_id.split('_')[1]}_b.{endwith[ide]}\",\n",
    "                    file_id = file_id,\n",
    "                        )\n",
    "            else:\n",
    "                #pass\n",
    "                try:\n",
    "                    shutil.move(f\"{default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id}.{endwith[ide]}\", f\"{default_path.replace('train', 'val')}/{folder}/\")\n",
    "                except:\n",
    "                    print(f\"{default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id}.{endwith[ide]}\")\n",
    "    for file_id in test:\n",
    "        for ide, folder in enumerate(folders):\n",
    "            if \"a\" in file_id:\n",
    "                pass\n",
    "            else:\n",
    "                #pass\n",
    "                try:\n",
    "                    shutil.move(f\"{default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id}.{endwith[ide]}\", f\"{default_path.replace('train', 'test')}/{folder}/\")\n",
    "                except:\n",
    "                    print(f\"{default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id}.{endwith[ide]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1_wayne_0_6_6.npy'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_data_path_npy = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_npy_partial2/\"\n",
    "all_data = os.listdir(f\"{ori_data_path_npy}{1}\")\n",
    "all_data[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_data_path_npy = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_npy_partial2/\"\n",
    "ori_data_path = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_v0.2.1_partial2/\"\n",
    "ori_data_path_ann = \"/data/nas07/PersonalData/danh/PantoMatrix/beat_english_v0.2.1_partial2/\"\n",
    "cache_path = f\"/data/nas07/PersonalData/danh/PantoMatrix/beat_4english_30_mixamo6/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [08:58, 44.86s/it]\n"
     ]
    }
   ],
   "source": [
    "npy_all = []\n",
    "for ii, file in tqdm(enumerate(all_data)):\n",
    "    file = file[:-4]\n",
    "    shutil.copy(f\"{ori_data_path_npy}/{file.split('_')[0]}/{file}.npy\", f\"{cache_path}{load_type}/wave16k/{file}.npy\")\n",
    "    try:\n",
    "        shutil.copy(f\"{ori_data_path}/{file.split('_')[0]}/{file}.TextGrid\", f\"{cache_path}{load_type}/text/{file}.TextGrid\")\n",
    "    except:\n",
    "        print(f\"{file}.TextGrid\")\n",
    "    try:\n",
    "        shutil.copy(f\"{ori_data_path_ann}/{file.split('_')[0]}/{file}.txt\", f\"{cache_path}{load_type}/sem/{file}.txt\")\n",
    "    except:\n",
    "        print(f\"{file}.txt\")\n",
    "    try:\n",
    "        shutil.copy(f\"{ori_data_path_ann}/{file.split('_')[0]}/{file}.csv\", f\"{cache_path}{load_type}/emo/{file}.csv\")\n",
    "    except:\n",
    "        print(f\"{file}.csv\")\n",
    "\n",
    "    npy_all.extend(list(np.load(f\"{ori_data_path_npy}/{file.split('_')[0]}/{file}.npy\")))\n",
    "\n",
    "    with open(f\"{ori_data_path}/{file.split('_')[0]}/{file}.json\", \"r\", encoding='utf-8') as json_file_raw:\n",
    "        json_file = json.load(json_file_raw)\n",
    "        with open(f\"{cache_path}{load_type}/facial52/{file}.json\", \"w\") as reduced_json:\n",
    "            counter = 0\n",
    "            new_frames_list = []\n",
    "            for json_data in json_file[\"frames\"]:\n",
    "                json_all.append(json_data[\"weights\"])\n",
    "                if counter % reduce_factor_json == 0:\n",
    "                    new_frames_list.append(json_data)\n",
    "                counter += 1\n",
    "            json_new = {\"names\": json_file[\"names\"], \"frames\": new_frames_list}\n",
    "            json.dump(json_new, reduced_json)\n",
    "\n",
    "        with open(f\"{ori_data_path}/{file.split('_')[0]}/{file}.bvh\", \"r\") as bvh_file:\n",
    "            with open(f\"{cache_path}{load_type}/bvh_rot/{file}.bvh\", \"w\") as reduced_raw_bvh:\n",
    "                with open(f\"{cache_path}{load_type}/bvh_full/{file}.bvh\", \"w\") as reduced_full_bvh:\n",
    "                    with open(f\"{cache_path}{load_type}/bvh_rot_vis/{file}.bvh\", \"w\") as reduced_trainable_bvh:\n",
    "                        for i, line_data in enumerate(bvh_file.readlines()):\n",
    "                            if i < 316: \n",
    "                                reduced_full_bvh.write(line_data)\n",
    "                                reduced_trainable_bvh.write(line_data)\n",
    "                            if i >= 316:\n",
    "                                data = np.fromstring(line_data, dtype=float, sep=' ')\n",
    "                                bvh_all.append(data)\n",
    "                                if i % reduce_factor_bvh == 0:\n",
    "                                    reduced_full_bvh.write(line_data)\n",
    "                                    trainable_rotation = np.zeros_like(data)\n",
    "                                    for k, v in target_list.items():\n",
    "                                        trainable_rotation[ori_list[k][1] - v:ori_list[k][1]] = data[ori_list[k][1] - v:ori_list[k][1]]\n",
    "\n",
    "                                    trainable_line_data = np.array2string(trainable_rotation, max_line_width=np.inf, precision=6, suppress_small=False, separator=' ')\n",
    "                                    reduced_trainable_bvh.write(trainable_line_data[1:-2] + \"\\n\")\n",
    "                                    data_rotation = np.zeros((1))   \n",
    "                                    for k, v in target_list.items():\n",
    "                                        data_rotation = np.concatenate((data_rotation, data[ori_list[k][1] - v:ori_list[k][1]]))                             \n",
    "                                        raw_line_data = np.array2string(data_rotation[1:], max_line_width=np.inf, precision=6, suppress_small=False, separator=' ')\n",
    "                                    reduced_raw_bvh.write(raw_line_data[1:-2] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #rot2pos\n",
    "# import time\n",
    "# start_t = time.time()\n",
    "# p = BVHParser()\n",
    "# data = [p.parse(\"../../../datasets/beat_full/1/1_wayne_0_1_8.bvh\")]\n",
    "# dr_pipe = Pipeline([\n",
    "#     ('param', MocapParameterizer('position')),\n",
    "# ])\n",
    "# xx = dr_pipe.fit_transform(data)\n",
    "# # data[0].values.shape\n",
    "# df = xx[0].values.head(-1)\n",
    "# print((time.time()-start_t)/60)\n",
    "# data_list = []\n",
    "# p_in_f = []\n",
    "# for joint, values in joint_list[\"beat_joints\"].items():\n",
    "#     x = df['%s_Xposition'%joint][-1]\n",
    "#     y = df['%s_Yposition'%joint][-1]\n",
    "#     z = df['%s_Zposition'%joint][-1]\n",
    "#     p = [x, y, z]\n",
    "#     p_in_f.append(p)\n",
    "# data_list.append(p_in_f)\n",
    "\n",
    "# %matplotlib inline\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# import matplotlib.pyplot as plt\n",
    "# draw = np.array(data_list[0])\n",
    "# fig = plt.figure()\n",
    "# ax = Axes3D(fig)\n",
    "# ax.scatter(draw.T[0], draw.T[1], draw.T[2], s=100, c='r')\n",
    "# plt.show()\n",
    "\n",
    "# json_str = {}\n",
    "# path = '../../../datasets/beat_full/'\n",
    "# for i in range(30):\n",
    "#     for bvh in os.listdir(path+str(i+1)+\"/\"):\n",
    "#         if bvh.endswith('.bvh'):\n",
    "#             p = BVHParser()\n",
    "#             filename = os.path.join(path, str(i+1), bvh)\n",
    "#             print(filename)\n",
    "#             data = [p.parse(filename)]\n",
    "\n",
    "#             dr_pipe = Pipeline([\n",
    "#                 ('param', MocapParameterizer('position')),\n",
    "#             ])\n",
    "\n",
    "#             xx = dr_pipe.fit_transform(data)\n",
    "#             data[0].values.shape\n",
    "#             df = xx[0].values.head(-1)\n",
    "\n",
    "#             data_list = []\n",
    "#             #counter = 0\n",
    "#             for f in range(data[0].values.shape[0]):\n",
    "#                 #if counter == 100: break\n",
    "#                 p_in_f = []\n",
    "#                 for joint, values in joint_list[\"beat_joints\"].items():\n",
    "#                     # print(joint)\n",
    "#                     x = df['%s_Xposition'%joint][f-1]\n",
    "#                     y = df['%s_Yposition'%joint][f-1]\n",
    "#                     z = df['%s_Zposition'%joint][f-1]\n",
    "#                     p = [x, y, z]\n",
    "#                     p_in_f.append(p)\n",
    "#                 #counter += 1\n",
    "#                 data_list.append(p_in_f)\n",
    "#             # print(np.array(data_list).shape)\n",
    "#             json_str[filename] = data_list\n",
    "#             print(json_str)\n",
    "#             # json_string = json.dumps(json_str)\n",
    "#             with open('position_data2.json', 'w') as outfile:\n",
    "#                 json.dump(json_str, outfile)\n",
    "#             break\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beat_preprocessing_kernel",
   "language": "python",
   "name": "beat_preprocessing_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
